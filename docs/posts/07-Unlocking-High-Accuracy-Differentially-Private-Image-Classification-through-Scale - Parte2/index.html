<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.251">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Alexander Zelada">
<meta name="dcterms.date" content="2022-12-09">

<title>Deep Learning y Privacidad - Unlocking High-Accuracy Differentially Private Image Classification through Scale - Parte 2</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Deep Learning y Privacidad</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">About</a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/lzeladam"><i class="bi bi-github" role="img">
</i> 
 </a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/lzeladam"><i class="bi bi-twitter" role="img">
</i> 
 </a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Unlocking High-Accuracy Differentially Private Image Classification through Scale - Parte 2</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">Privacy</div>
                <div class="quarto-category">Paper</div>
                <div class="quarto-category">Privacy-preserving</div>
                <div class="quarto-category">DeepMind</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Alexander Zelada </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">December 9, 2022</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<blockquote class="blockquote">
<p>(DP-SGD), el m√©todo de entrenamiento de DP m√°s popular para el aprendizaje profundo, realiza esta protecci√≥n mediante la inyecci√≥n de ruido durante el entrenamiento.</p>
</blockquote>
<p>Voy a trabajar en la traducci√≥n del paper <a href="https://www.deepmind.com/blog/unlocking-high-accuracy-differentially-private-image-classification-through-scale">‚ÄúUnlocking High-Accuracy Differentially Private Image Classification through Scale‚Äù</a>, este documento consta de 6 secciones y cada post ser√° una de ellas. Como parte de mi aprendizaje considero importante generar informaci√≥n en mi lengua materna.</p>
<section id="background" class="level2">
<h2 class="anchored" data-anchor-id="background"><strong>2.- Background</strong></h2>
<p><strong>2.1. Privacidad Diferencial DP</strong></p>
<p>La privacidad diferencial es una garant√≠a de privacidad formal que se aplica a los algoritmos de an√°lisis de datos aleatorios. Por construcci√≥n, los algoritmos diferencialmente privados evitan que un adversario que observa el resultado de un c√≥mputo deduzca cualquier propiedad relacionada con data points individuales en los datos de entrada utilizados durante el c√≥mputo.</p>
<p>La fuerza de esta garant√≠a est√° controlada por dos par√°metros: <span class="math inline">\(\epsilon&gt;0\)</span> y <span class="math inline">\(\delta \in [0,1]\)</span>. En t√©rminos generales, <span class="math inline">\(\epsilon\)</span> limita la relaci√≥n logar√≠tmica de verosimilitud de cualquier resultado particular que se puede obtener al ejecutar el algoritmo en dos conjuntos de datos que difieren en un solo punto de datos, y <span class="math inline">\(\delta\)</span> es una peque√±a probabilidad que limita la aparici√≥n de resultados poco frecuentes que violan este l√≠mite. La garant√≠a de privacidad se fortalece a medida que ambos par√°metros se reducen. Una regla general est√°ndar establece que, para obtener una privacidad significativa, <span class="math inline">\(\epsilon\)</span> debe ser una constante peque√±a mientras que <span class="math inline">\(\delta\)</span> debe ser menor que <span class="math inline">\(1/N\)</span>, donde <span class="math inline">\(N\)</span> es el tama√±o del conjunto de datos de entrada. M√°s formalmente, tenemos lo siguiente.</p>
<p><strong>Definici√≥n 2.1</strong> (Differential Privacy (<a href="https://people.csail.mit.edu/asmith/PS/sensitivity-tcc-final.pdf">Dwork et al., 2006</a>)). Sea <span class="math inline">\(A:D \longmapsto S\)</span> un algoritmo aleatorio, y sea : <span class="math inline">\(\epsilon&gt;0\)</span> y <span class="math inline">\(\delta \in [0,1]\)</span>. Decimos que <span class="math inline">\(A\)</span> es <span class="math inline">\((\epsilon, \delta)-DP\)</span> si para dos conjuntos de datos vecinos cualesquiera <span class="math inline">\(D, D{'} \in D\)</span> difieren en un solo elemento, tenemos que:</p>
<p><span class="math display">\[
\begin{equation} \lor S \subset S, P[A(D)\in S] \leq exp(\epsilon)P[A(D{'})\in S] + \delta  \end{equation}
\]</span></p>
<p>La protecci√≥n de la privacidad que brinda DP se mantiene bajo un modelo de amenaza extremadamente fuerte: las inferencias sobre las personas est√°n protegidas incluso frente a un adversario que tiene pleno conocimiento del algoritmo DP, poder computacional ilimitado y <em>arbitrary side knowledge</em> sobre los datos de entrada. Adem√°s, DP satisface una serie de propiedades atractivas desde el punto de vista del dise√±o de algoritmos, incluida la conservaci√≥n bajo procesamiento posterior y una degradaci√≥n suave con m√∫ltiple accesos a los mismos datos. Estas propiedades se explotan en la construcci√≥n de algoritmos DP complejos basados en la combinaci√≥n de peque√±os bloques de construcci√≥n que inyectan ruido cuidadosamente calibrado en las operaciones que acceden a los datos. La magnitud del ruido requerido para satisfacer la garant√≠a de privacidad aumenta con la fuerza de los par√°metros de privacidad, lo que lleva a una compensaci√≥n inevitable entre utilidad y privacidad, como lo ilustra la Ley Fundamental de Recuperaci√≥n de Informaci√≥n <a href="https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf">(Dwork and Roth, 2014)</a>.</p>
<p>Juntos, la solidez de la garant√≠a formal que brinda y la variedad de herramientas disponibles para la construcci√≥n de algoritmos de DP han llevado a la creciente adopci√≥n de DP como un est√°ndar de oro para el aprendizaje autom√°tico que preserva la privacidad. Para problemas de <em>convex learning</em>, existe una variedad de m√©todos para obtener algoritmos diferencialmente privados, incluida la perturbaci√≥n de salida <a href="https://www.jmlr.org/papers/volume12/chaudhuri11a/chaudhuri11a.pdf">(Chaudhuri et al., 2011</a>; <a href="https://arxiv.org/pdf/1606.04722.pdf">Wu et al., 2017</a>), la perturbaci√≥n objetiva (<a href="https://www.jmlr.org/papers/volume12/chaudhuri11a/chaudhuri11a.pdf">Chaudhuri et al., 2011</a>; <a href="http://proceedings.mlr.press/v23/kifer12/kifer12.pdf">Kifer et al., 2012</a>) y la perturbaci√≥n de gradiente (<a href="https://arxiv.org/pdf/1405.7085.pdf">Bassily et al., 2014</a>; <a href="https://cseweb.ucsd.edu/~kamalika/pubs/scs13.pdf">Song et al., 2013</a>) .</p>
<p>La naturaleza de los problemas convexos permite el an√°lisis formal de la utilidad de privacidad que ofrecen estos algoritmos, y en la actualidad existen grandes clases de problemas para los cuales se conocen algoritmos que logran (casi) equilibrios √≥ptimos de utilidad de privacidad (<a href="https://arxiv.org/pdf/2103.01516.pdf">Asi et al., 2021</a>; <a href="https://arxiv.org/pdf/1405.7085.pdf">Bassily et al., 2014</a>; <a href="https://arxiv.org/pdf/2005.04763.pdf">Feldman et al., 2020</a>; <a href="https://proceedings.mlr.press/v130/song21a.html">Song et al., 2021</a>; <a href="https://papers.nips.cc/paper/2015/file/52d080a3e172c33fd6886a37e7288491-Paper.pdf">Talwar et al., 2015</a>). Para los problemas de aprendizaje no convexos, la gama de algoritmos disponibles es m√°s limitada y las ventajas y desventajas de la privacidad y la utilidad son m√°s dif√≠ciles de analizar te√≥ricamente. No obstante, para tales problemas existen dos familias de algoritmos que han demostrado lograr compensaciones razonables de privacidad-utilidad-c√≥mputo en la pr√°ctica; perturbaci√≥n de gradiente aplicada a optimizadores est√°ndar como SGD (<a href="https://arxiv.org/pdf/1607.00133.pdf">Abadi et al., 2016</a>), y agregaci√≥n privada de <em>teacher ensembles (<a href="https://arxiv.org/pdf/1802.08908.pdf">Papernot et al., 2018</a>).</em> En este trabajo nos centramos en el primero, que es el m√°s utilizado.</p>
<p><strong>2.2.</strong> <strong>Descenso de gradiente estoc√°stico diferencialmente privado (DP-SGD)</strong></p>
<p>En este trabajo, suponemos que el algoritmo diferencialmente privado A, es un algoritmo de aprendizaje que mapea un conjunto de datos de entrenamiento: <span class="math inline">\(D = \lbrace(x_i, y_i)\rbrace_{1\leq i\leq N}\)</span> a un vector de par√°metros de la red neuronal aprendida <span class="math inline">\(w \in S = R^{p}\)</span>. Sea <span class="math inline">\(\mathcal{L}(w,x,y)\)</span> el objetivo de aprendizaje (por ejemplo, la p√©rdida de entrop√≠a cruzada), dados los par√°metros del modelo <span class="math inline">\(w\)</span>, la entrada ejemplo <span class="math inline">\(x\)</span> y la etiqueta <span class="math inline">\(y\)</span>. Por comodidad, utilizamos la notaci√≥n abreviada <span class="math inline">\(l_i(w) = \mathcal{L}(w,x_i,y_i)\)</span>.</p>
<p>En la configuraci√≥n non-private, una actualizaci√≥n de par√°metros utilizando el Descenso de Gradiente Estoc√°stico (SGD) en la iteraci√≥n <span class="math inline">\(t\)</span> extrae <span class="math inline">\(B\)</span> ejemplos al azar del conjuntos de datos, y realiza una actualizaci√≥n de la forma:</p>
<p><span class="math display">\[
w^{(t+1)}= w^{(t)} - \eta_{t}\frac{1}{B} \sum_{i \in \beta_t}\nabla l_i(w^{(t)}),
\]</span></p>
<p>Donde <span class="math inline">\(\eta_t\)</span> es el step-size para una actualizaci√≥n <span class="math inline">\(t^{th}\)</span>, <span class="math inline">\(\nabla\)</span> denota el operador de gradiente, y <span class="math inline">\(B_t\)</span> representa el conjunto de ejemplos muestreados en la iteraci√≥n <span class="math inline">\(t\)</span> con <span class="math inline">\(|B_t| = B\)</span>. Para que este algoritmo sea diferencialmente privado, aplicamos las siguientes modificaciones. En primer lugar, el gradiente de cada ejemplo del mini-batch es clipped a una norma m√°xima <span class="math inline">\(C\)</span>, y en segundo lugar, se a√±ade ruido gaussiano con desviaci√≥n est√°ndar proporcional a <span class="math inline">\(C\)</span> se a√±ade a la media de los grandientes clipped.</p>
<p>Sea</p>
<p><span class="math display">\[
clip_x:v\in R^{p}\longmapsto min\lbrace{1,\frac{c}{||v||_2}\rbrace}.v\in R^{p}
\]</span></p>
<p>denote la funci√≥n de clipping que reescala su entrada para que la salida tenga una norma m√°xima <span class="math inline">\(l_2\)</span> de <span class="math inline">\(C\)</span>. El nuevo step de actualizaci√≥n es:</p>
<p><span class="math display">\[
\begin{equation}  w^{(t+1)}= w^{(t)} -\eta_{t} \{\frac{1}{B}\sum_{i \in \beta_t}clip_c\left(\nabla l_i(w^{(t)})\right) +  \frac{\sigma C}{B}\xi\}, \end{equation}
\]</span></p>
<p>Donde <span class="math inline">\(\xi \sim N(0,I_p)\)</span> es una variable aleatoria gaussiana est√°ndar de <span class="math inline">\(p\)</span> dimensiones y <span class="math inline">\(\sigma\)</span> especifica la desviaci√≥n est√°ndar del ruido a√±adido. El algoritmo resultante se llama <strong><em>Differentially Private-Stochastic Gradient Descent (DP-SGD)</em></strong> (<a href="https://arxiv.org/pdf/1607.00133.pdf">Abadi et al., 2016</a>). Intuitivamente, realizar una actualizaci√≥n del modelo utilizando la ecuaci√≥n proporciona privacidad diferencial porque la adici√≥n de ruido gaussiano con desviaci√≥n est√°ndar proporcional a <span class="math inline">\(C\)</span> es suficiente para enmascarar la contribuci√≥n de cualquier ejemplo individual cuyo gradiente recortado tenga una norma menor o igual a <span class="math inline">\(C\)</span>. Aunque utilizamos el SGD privatizado como nuestro optimizador a lo largo de este trabajo, tambi√©n se puede utilizar un m√©todo de privatizaci√≥n similar en combinaci√≥n con otros algoritmos de optimizaci√≥n de primer orden, como SGD con momentum o Adam (<a href="https://arxiv.org/pdf/1812.06210.pdf">McMahan et al., 2018a</a>).</p>
<p>A lo largo de este trabajo utilizamos una versi√≥n modificada de DP-SGD en la que el gradiente privatizado est√° normalizado por <span class="math inline">\(C\)</span>:</p>
<p><span class="math display">\[
\begin{equation} w^{(t+1)}= w^{(t)} -\eta_{t} \{\frac{1}{B}\sum_{i \in \beta_t}\frac{1}{C} clip_c\left(\nabla l_i(w^{(t)})\right) + \frac{\sigma}{B}\xi\}, \end{equation}
\]</span></p>
<p>Esta es una reparametrizaci√≥n de la ecuaci√≥n (2) en la que la tasa de aprendizaje <span class="math inline">\(\eta_t\)</span> absorbe un factor de <span class="math inline">\(C\)</span>. Esto no tiene ning√∫n efecto sobre las garant√≠as de privacidad, pero asegura que la norma de clipping no influya en la escala de la actualizaci√≥n, lo que simplifica el ajuste de los hiperpar√°metros. Tenga en cuenta que para preservar las garant√≠as de DP, debemos dividir por <span class="math inline">\(C\)</span> despu√©s de la operaci√≥n de clipping. El Ap√©ndice A.1 proporciona m√°s detalles sobre nuestra implementaci√≥n de DP-SGD, incluida una descripci√≥n de nuestro enfoque de procesamiento por virtual batching para permitir el entrenamiento con grandes batch sizes.</p>
<p><strong>Privacy accounting.</strong> La garant√≠a de privacidad de DP-SGD est√° determinada por tres par√°metros: la desviaci√≥n est√°ndar <span class="math inline">\(\sigma\)</span>, el ratio de muestreo <span class="math inline">\(q=B/N\)</span> y el n√∫mero de iteraciones de entrenamiento <span class="math inline">\(T\)</span>. En la pr√°ctica, el presupuesto de privacidad <span class="math inline">\((\epsilon, \delta)\)</span> suele ser fijo, y estos tres hiperpar√°metros se eligen para proporcionar el mejor rendimiento posible dentro de este presupuesto. Tambi√©n puede haber restricciones pr√°cticas adicionales (por ejemplo, el presupuesto de c√°lculo m√°ximo disponible). El proceso de calibraci√≥n de la privacidad se realiza mediante un contador de privacidad: un algoritmo num√©rico que proporciona l√≠mites superiores ajustados para el presupuesto de privacidad en funci√≥n de los hiperpar√°metros (<a href="https://arxiv.org/pdf/1607.00133.pdf">Abadi et al., 2016</a>), que a su vez puede combinarse con rutinas de optimizaci√≥n num√©rica para optimizar un hip√©rparametro dado el presupuesto de privacidad y los otros dos hiperpar√°metros.</p>
<p>En este trabajo utilizamos el m√©todo de contabilidad para DP-SGD propuesto por M<a href="https://arxiv.org/pdf/1908.10530.pdf">ironov et al.&nbsp;(2019)</a> e implementado en TensorFlow Privacy (<a href="https://github.com/tensorflow/privacy">Google, 2018</a>). Esta privacidad contable se basa en un an√°lisis de ‚Äúcomposici√≥n‚Äù a trav√©s de iteraciones, que nos permite liberar no solo el modelo final, sino tambi√©n cada modelo intermedio obtenido durante el entrenamiento (bajo el mismo presupuesto de privacidad)</p>
<p><strong>2.3. Retos de DP-SGD</strong></p>
<p>Como describimos anteriormente, existen tres diferencias clave entre DP-SGD y SGD no privado: (1) Los gradientes por ejemplo se recortan (clipped) a una norma m√°xima de <span class="math inline">\(l_2\)</span> antes de promediarlos, (2) se a√±ade ruido gaussiano a la media de los gradientes clipped, y (3) el n√∫mero m√°ximo de actualizaciones permitidas dentro del presupuesto de privacidad est√° limitado y depende del batch size/ruido a√±adido. Estas diferencias plantean una serie de retos:</p>
<p><strong>Ajuste y regularizaci√≥n de hiperpar√°metros.</strong> El ruido agregado a la estimaci√≥n del gradiente en la actualizaci√≥n de DP-SGD (Ecuaci√≥n (3)) es una barrera significativa para la optimizaci√≥n eficiente, y si reducimos la escala de este ruido, el n√∫mero de iteraciones de entrenamiento permitidas dentro del presupuesto de privacidad disminuye. Esta restricci√≥n altera los valores √≥ptimos de hiperpar√°metros clave como el batch size/learning rate, y los valores por defecto del entrenamiento no privado pueden ser muy sub√≥ptimos (<a href="https://arxiv.org/pdf/2007.14191.pdf">Papernot et al., 2021</a>). En consecuencia, DP-SGD requiere un ajuste cuidadoso de los hiperpar√°metros.</p>
<p>En nuestros experimentos tambi√©n descubrimos que, al entrenar con DP-SGD, las mejoras en el training accuracy suelen traducirse directamente en una mejora de la generalizaci√≥n, sin necesidad de una fuerte regularizaci√≥n. Inspirados en esta observaci√≥n, nuestra filosof√≠a es que los m√©todos que reducen el n√∫mero de iteraciones de entrenamiento necesarias para alcanzar un high training accuracy en el entrenamiento no privado probablemente mejoren el test accuracy alcanzado en el entrenamiento privado. De acuerdo con este enfoque, suele ser beneficioso eliminar los m√©todos de regularizaci√≥n expl√≠citos.</p>
<p><strong>Bias y Varianza de la actualizaci√≥n DP-SGD.</strong> El estimador de gradiente utilizado por DP-SGD est√° biased debido al uso de gradiente clipping por ejemplo y, en general, no corresponde al gradiente de ninguna funci√≥n diferenciable (<a href="https://proceedings.mlr.press/v130/song21a.html">Song et al., 2021</a>). Y lo que es m√°s importante, la norma de clipping <span class="math inline">\(C\)</span> introduce una compensaci√≥n entre sesgo y varianza (<a href="https://arxiv.org/pdf/2006.15429.pdf">Chen et al., 2020</a>; <a href="https://arxiv.org/pdf/1905.03871.pdf">Thakkar et al., 2021</a>). Esto puede verse en la actualizaci√≥n DP-SGD que se muestra en la ecuaci√≥n (3). Cuando <span class="math inline">\(C\)</span> es muy grande, <span class="math inline">\(clip_c\)</span> es la funci√≥n identidad, por lo que el gradiente privatizado es un estimador unbiased del verdadero gradiente, pero el gradiente clipped <span class="math inline">\(\frac {1}{c} clip_c(\nabla l_i(w^{(t)}))\)</span> es muy peque√±o en comparaci√≥n con el ruido (que es independiente de <span class="math inline">\(C\)</span>) - en general, la estimaci√≥n del gradiente privatizado tiene un bias bajo y una varianza alta. Por el contrario, si <span class="math inline">\(C\)</span> es peque√±o, la operaci√≥n de clipping introduce un bias, pero el gradiente clipped <span class="math inline">\(\frac {1}{c} clip_c(\nabla l_i(w^{(t)}))\)</span> es mayor, y por lo tanto no es necesariamente peque√±o en comparacion con el ruido- en general, la estimaci√≥n del gradiente privatizado tiene un bias alto y una varianza baja. Tenga en cuenta que cuando <span class="math inline">\(C\)</span> es muy peque√±o (m√°s peque√±o que la norma de gradiente m√°s peque√±a por ejemplo), reducir a√∫n m√°s <span class="math inline">\(C\)</span> no cambia <span class="math inline">\(\frac {1}{c} clip_c(\nabla l_i(w^{(t)}))\)</span>, lo que indica que el bias y la varianza en la actualizaci√≥n se aproximan a una constante a medida que <span class="math inline">\(C \longmapsto 0\)</span>. Curiosamente, trabajos anteriores han observado que amplios rangos de la norma de clipping <span class="math inline">\(C\)</span> pueden proporcionar un rendimiento casi √≥ptimo siempre que (i) la norma de clipping sea lo suficientemente peque√±a, y (ii) el learning-rate se reescalen en consecuecia <a href="https://arxiv.org/pdf/2201.12328.pdf">(Kurakin et al., 2022</a>; <a href="https://arxiv.org/pdf/2110.05679.pdf">Li et al., 2021</a>). Esto sugiere que reducir la varianza introducida por el ruido puede ser m√°s importante que reducir el bias introducido por el clipping.</p>
<p><strong>Hacer que los modelos est√°ndar funcionen.</strong> El entrenamiento diferencialmente privado ha obtenido recientemente resultados prometedores con arquitecturas est√°ndar en NLP, tanto al entrenar un modelo BERT (<a href="https://arxiv.org/pdf/1810.04805.pdf">Devlin et al., 2018</a>) a partir de una inicializaci√≥n aleatoria (<a href="https://arxiv.org/pdf/2108.01624.pdf">Anil et al., 2018</a>) como al afinar un gran modelo de lenguaje Transformer (<a href="https://arxiv.org/pdf/1706.03762.pdf">Vaswani et al., 2017</a>) a partir de un conjunto de par√°metros preentrenados (<a href="https://arxiv.org/pdf/2110.05679.pdf">Li et al., 2021</a>; <a href="https://arxiv.org/pdf/2110.06500.pdf">Yu et al., 2021a</a>). Sin embargo, no se han obtenido resultados similares en visi√≥n por ordenador, y la bibliograf√≠a no ofrece recomendaciones claras sobre qu√© arquitecturas de modelos funcionan bien. Por ejemplo, analizando la investigaci√≥n reciente sobre el entrenamiento privado para CIFAR-10, <a href="https://arxiv.org/pdf/2201.12328.pdf">Kurakin et al.&nbsp;(2022)</a>, <a href="https://arxiv.org/pdf/2007.14191.pdf">Papernot et al.&nbsp;(2021)</a> y <a href="https://arxiv.org/pdf/2110.06255.pdf">D√∂rmann et al.&nbsp;(2021)</a> utilizan variantes de modelos VGG poco profundos (<a href="https://arxiv.org/pdf/1409.1556.pdf">Simonyan y Zisserman, 2015</a>), mientras que <a href="https://arxiv.org/pdf/2011.11660.pdf">Tram√®r y Boneh (2021)</a> utilizan ScatterNets (<a href="https://arxiv.org/pdf/1412.8659.pdf">Oyallon y Mallat, 2015</a>) para entrenar modelos lineales en caracter√≠sticas artesanales, logrando una impresionante precisi√≥n de prueba del 69,3 % con un presupuesto de privacidad ajustado de <span class="math inline">\((3, 10^{-5})-DP\)</span>. Por √∫ltimo, <a href="https://arxiv.org/pdf/2203.00324.pdf">Klause et al.&nbsp;(2022)</a> logran la precisi√≥n de prueba SOTA para ùúÄ ‚â§ 8 sin datos adicionales del 71,7% al entrenar una red residual superficial de 9 capas <a href="https://arxiv.org/pdf/1512.03385.pdf">(He et al., 2016</a>) bajo <span class="math inline">\((7,5, 10^{-5})-DP\)</span>.</p>
<p>La norma <span class="math inline">\(l_2\)</span> del ruido a√±adido en la actualizaci√≥n DP-SGD escala proporcionalmente a la dimensi√≥n del gradiente (el n√∫mero de par√°metros). Esta observaci√≥n ha llevado a muchos investigadores a creer que los modelos est√°ndar sobreparametrizados funcionar√°n mal con DP-SGD, y en su lugar se centran en reducir la dimensi√≥n expl√≠cita o impl√≠cita de la actualizaci√≥n, ya sea mediante el uso de modelos peque√±os/hand-crafted features (<a href="https://arxiv.org/pdf/2011.11660.pdf">Tram√®r y Boneh, 2021</a>) o mediante t√©cnicas de reducci√≥n de la dimensionalidad (<a href="https://arxiv.org/pdf/2102.12677.pdf">Yu et al., 2021b,c</a>). Otro obst√°culo clave para el uso de modelos est√°ndar para el entrenamiento privado en visi√≥n por computador ha sido que, con el fin de proporcionar garant√≠as ajustadas de DP, DP-SGD requiere que los gradientes evaluados en diferentes ejemplos de entrenamiento sean independientes. Esto excluye el uso de cualquier m√©todo que permita la comunicaci√≥n entre ejemplos de entrenamiento, como batch normalization (<a href="https://arxiv.org/pdf/1502.03167.pdf">Ioffe y Szegedy, 2015</a>), que hasta hace poco ha sido casi omnipresente en las arquitecturas de visi√≥n est√°ndar (<a href="https://arxiv.org/pdf/2102.06171.pdf">Brock et al., 2021b</a>; <a href="https://arxiv.org/pdf/2010.11929.pdf">Dosovitskiy et al., 2020</a>; <a href="https://arxiv.org/pdf/1512.03385.pdf">He et al., 2016</a>; <a href="https://arxiv.org/pdf/1905.11946.pdf">Tan y Le, 2019</a>; <a href="https://arxiv.org/pdf/1605.07146.pdf">Zagoruyko y Komodakis, 2016</a>).</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>