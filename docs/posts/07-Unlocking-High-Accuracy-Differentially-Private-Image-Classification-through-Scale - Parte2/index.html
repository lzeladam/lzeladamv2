<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.251">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Alexander Zelada">
<meta name="dcterms.date" content="2022-12-09">

<title>Deep Learning y Privacidad - Unlocking High-Accuracy Differentially Private Image Classification through Scale - Parte 2</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Deep Learning y Privacidad</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">About</a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/lzeladam"><i class="bi bi-github" role="img">
</i> 
 </a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/lzeladam"><i class="bi bi-twitter" role="img">
</i> 
 </a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Unlocking High-Accuracy Differentially Private Image Classification through Scale - Parte 2</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">Privacy</div>
                <div class="quarto-category">Paper</div>
                <div class="quarto-category">Privacy-preserving</div>
                <div class="quarto-category">DeepMind</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Alexander Zelada </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">December 9, 2022</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<blockquote class="blockquote">
<p>(DP-SGD), el método de entrenamiento de DP más popular para el aprendizaje profundo, realiza esta protección mediante la inyección de ruido durante el entrenamiento.</p>
</blockquote>
<p>Voy a trabajar en la traducción del paper <a href="https://www.deepmind.com/blog/unlocking-high-accuracy-differentially-private-image-classification-through-scale">“Unlocking High-Accuracy Differentially Private Image Classification through Scale”</a>, este documento consta de 6 secciones y cada post será una de ellas. Como parte de mi aprendizaje considero importante generar información en mi lengua materna.</p>
<section id="background" class="level2">
<h2 class="anchored" data-anchor-id="background"><strong>2.- Background</strong></h2>
<p><strong>2.1. Privacidad Diferencial DP</strong></p>
<p>La privacidad diferencial es una garantía de privacidad formal que se aplica a los algoritmos de análisis de datos aleatorios. Por construcción, los algoritmos diferencialmente privados evitan que un adversario que observa el resultado de un cómputo deduzca cualquier propiedad relacionada con data points individuales en los datos de entrada utilizados durante el cómputo.</p>
<p>La fuerza de esta garantía está controlada por dos parámetros: <span class="math inline">\(\epsilon&gt;0\)</span> y <span class="math inline">\(\delta \in [0,1]\)</span>. En términos generales, <span class="math inline">\(\epsilon\)</span> limita la relación logarítmica de verosimilitud de cualquier resultado particular que se puede obtener al ejecutar el algoritmo en dos conjuntos de datos que difieren en un solo punto de datos, y <span class="math inline">\(\delta\)</span> es una pequeña probabilidad que limita la aparición de resultados poco frecuentes que violan este límite. La garantía de privacidad se fortalece a medida que ambos parámetros se reducen. Una regla general estándar establece que, para obtener una privacidad significativa, <span class="math inline">\(\epsilon\)</span> debe ser una constante pequeña mientras que <span class="math inline">\(\delta\)</span> debe ser menor que <span class="math inline">\(1/N\)</span>, donde <span class="math inline">\(N\)</span> es el tamaño del conjunto de datos de entrada. Más formalmente, tenemos lo siguiente.</p>
<p><strong>Definición 2.1</strong> (Differential Privacy (<a href="https://people.csail.mit.edu/asmith/PS/sensitivity-tcc-final.pdf">Dwork et al., 2006</a>)). Sea <span class="math inline">\(A:D \longmapsto S\)</span> un algoritmo aleatorio, y sea : <span class="math inline">\(\epsilon&gt;0\)</span> y <span class="math inline">\(\delta \in [0,1]\)</span>. Decimos que <span class="math inline">\(A\)</span> es <span class="math inline">\((\epsilon, \delta)-DP\)</span> si para dos conjuntos de datos vecinos cualesquiera <span class="math inline">\(D, D{'} \in D\)</span> difieren en un solo elemento, tenemos que:</p>
<p><span class="math display">\[
\begin{equation} \lor S \subset S, P[A(D)\in S] \leq exp(\epsilon)P[A(D{'})\in S] + \delta  \end{equation}
\]</span></p>
<p>La protección de la privacidad que brinda DP se mantiene bajo un modelo de amenaza extremadamente fuerte: las inferencias sobre las personas están protegidas incluso frente a un adversario que tiene pleno conocimiento del algoritmo DP, poder computacional ilimitado y <em>arbitrary side knowledge</em> sobre los datos de entrada. Además, DP satisface una serie de propiedades atractivas desde el punto de vista del diseño de algoritmos, incluida la conservación bajo procesamiento posterior y una degradación suave con múltiple accesos a los mismos datos. Estas propiedades se explotan en la construcción de algoritmos DP complejos basados en la combinación de pequeños bloques de construcción que inyectan ruido cuidadosamente calibrado en las operaciones que acceden a los datos. La magnitud del ruido requerido para satisfacer la garantía de privacidad aumenta con la fuerza de los parámetros de privacidad, lo que lleva a una compensación inevitable entre utilidad y privacidad, como lo ilustra la Ley Fundamental de Recuperación de Información <a href="https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf">(Dwork and Roth, 2014)</a>.</p>
<p>Juntos, la solidez de la garantía formal que brinda y la variedad de herramientas disponibles para la construcción de algoritmos de DP han llevado a la creciente adopción de DP como un estándar de oro para el aprendizaje automático que preserva la privacidad. Para problemas de <em>convex learning</em>, existe una variedad de métodos para obtener algoritmos diferencialmente privados, incluida la perturbación de salida <a href="https://www.jmlr.org/papers/volume12/chaudhuri11a/chaudhuri11a.pdf">(Chaudhuri et al., 2011</a>; <a href="https://arxiv.org/pdf/1606.04722.pdf">Wu et al., 2017</a>), la perturbación objetiva (<a href="https://www.jmlr.org/papers/volume12/chaudhuri11a/chaudhuri11a.pdf">Chaudhuri et al., 2011</a>; <a href="http://proceedings.mlr.press/v23/kifer12/kifer12.pdf">Kifer et al., 2012</a>) y la perturbación de gradiente (<a href="https://arxiv.org/pdf/1405.7085.pdf">Bassily et al., 2014</a>; <a href="https://cseweb.ucsd.edu/~kamalika/pubs/scs13.pdf">Song et al., 2013</a>) .</p>
<p>La naturaleza de los problemas convexos permite el análisis formal de la utilidad de privacidad que ofrecen estos algoritmos, y en la actualidad existen grandes clases de problemas para los cuales se conocen algoritmos que logran (casi) equilibrios óptimos de utilidad de privacidad (<a href="https://arxiv.org/pdf/2103.01516.pdf">Asi et al., 2021</a>; <a href="https://arxiv.org/pdf/1405.7085.pdf">Bassily et al., 2014</a>; <a href="https://arxiv.org/pdf/2005.04763.pdf">Feldman et al., 2020</a>; <a href="https://proceedings.mlr.press/v130/song21a.html">Song et al., 2021</a>; <a href="https://papers.nips.cc/paper/2015/file/52d080a3e172c33fd6886a37e7288491-Paper.pdf">Talwar et al., 2015</a>). Para los problemas de aprendizaje no convexos, la gama de algoritmos disponibles es más limitada y las ventajas y desventajas de la privacidad y la utilidad son más difíciles de analizar teóricamente. No obstante, para tales problemas existen dos familias de algoritmos que han demostrado lograr compensaciones razonables de privacidad-utilidad-cómputo en la práctica; perturbación de gradiente aplicada a optimizadores estándar como SGD (<a href="https://arxiv.org/pdf/1607.00133.pdf">Abadi et al., 2016</a>), y agregación privada de <em>teacher ensembles (<a href="https://arxiv.org/pdf/1802.08908.pdf">Papernot et al., 2018</a>).</em> En este trabajo nos centramos en el primero, que es el más utilizado.</p>
<p><strong>2.2.</strong> <strong>Descenso de gradiente estocástico diferencialmente privado (DP-SGD)</strong></p>
<p>En este trabajo, suponemos que el algoritmo diferencialmente privado A, es un algoritmo de aprendizaje que mapea un conjunto de datos de entrenamiento: <span class="math inline">\(D = \lbrace(x_i, y_i)\rbrace_{1\leq i\leq N}\)</span> a un vector de parámetros de la red neuronal aprendida <span class="math inline">\(w \in S = R^{p}\)</span>. Sea <span class="math inline">\(\mathcal{L}(w,x,y)\)</span> el objetivo de aprendizaje (por ejemplo, la pérdida de entropía cruzada), dados los parámetros del modelo <span class="math inline">\(w\)</span>, la entrada ejemplo <span class="math inline">\(x\)</span> y la etiqueta <span class="math inline">\(y\)</span>. Por comodidad, utilizamos la notación abreviada <span class="math inline">\(l_i(w) = \mathcal{L}(w,x_i,y_i)\)</span>.</p>
<p>En la configuración non-private, una actualización de parámetros utilizando el Descenso de Gradiente Estocástico (SGD) en la iteración <span class="math inline">\(t\)</span> extrae <span class="math inline">\(B\)</span> ejemplos al azar del conjuntos de datos, y realiza una actualización de la forma:</p>
<p><span class="math display">\[
w^{(t+1)}= w^{(t)} - \eta_{t}\frac{1}{B} \sum_{i \in \beta_t}\nabla l_i(w^{(t)}),
\]</span></p>
<p>Donde <span class="math inline">\(\eta_t\)</span> es el step-size para una actualización <span class="math inline">\(t^{th}\)</span>, <span class="math inline">\(\nabla\)</span> denota el operador de gradiente, y <span class="math inline">\(B_t\)</span> representa el conjunto de ejemplos muestreados en la iteración <span class="math inline">\(t\)</span> con <span class="math inline">\(|B_t| = B\)</span>. Para que este algoritmo sea diferencialmente privado, aplicamos las siguientes modificaciones. En primer lugar, el gradiente de cada ejemplo del mini-batch es clipped a una norma máxima <span class="math inline">\(C\)</span>, y en segundo lugar, se añade ruido gaussiano con desviación estándar proporcional a <span class="math inline">\(C\)</span> se añade a la media de los grandientes clipped.</p>
<p>Sea</p>
<p><span class="math display">\[
clip_x:v\in R^{p}\longmapsto min\lbrace{1,\frac{c}{||v||_2}\rbrace}.v\in R^{p}
\]</span></p>
<p>denote la función de clipping que reescala su entrada para que la salida tenga una norma máxima <span class="math inline">\(l_2\)</span> de <span class="math inline">\(C\)</span>. El nuevo step de actualización es:</p>
<p><span class="math display">\[
\begin{equation}  w^{(t+1)}= w^{(t)} -\eta_{t} \{\frac{1}{B}\sum_{i \in \beta_t}clip_c\left(\nabla l_i(w^{(t)})\right) +  \frac{\sigma C}{B}\xi\}, \end{equation}
\]</span></p>
<p>Donde <span class="math inline">\(\xi \sim N(0,I_p)\)</span> es una variable aleatoria gaussiana estándar de <span class="math inline">\(p\)</span> dimensiones y <span class="math inline">\(\sigma\)</span> especifica la desviación estándar del ruido añadido. El algoritmo resultante se llama <strong><em>Differentially Private-Stochastic Gradient Descent (DP-SGD)</em></strong> (<a href="https://arxiv.org/pdf/1607.00133.pdf">Abadi et al., 2016</a>). Intuitivamente, realizar una actualización del modelo utilizando la ecuación proporciona privacidad diferencial porque la adición de ruido gaussiano con desviación estándar proporcional a <span class="math inline">\(C\)</span> es suficiente para enmascarar la contribución de cualquier ejemplo individual cuyo gradiente recortado tenga una norma menor o igual a <span class="math inline">\(C\)</span>. Aunque utilizamos el SGD privatizado como nuestro optimizador a lo largo de este trabajo, también se puede utilizar un método de privatización similar en combinación con otros algoritmos de optimización de primer orden, como SGD con momentum o Adam (<a href="https://arxiv.org/pdf/1812.06210.pdf">McMahan et al., 2018a</a>).</p>
<p>A lo largo de este trabajo utilizamos una versión modificada de DP-SGD en la que el gradiente privatizado está normalizado por <span class="math inline">\(C\)</span>:</p>
<p><span class="math display">\[
\begin{equation} w^{(t+1)}= w^{(t)} -\eta_{t} \{\frac{1}{B}\sum_{i \in \beta_t}\frac{1}{C} clip_c\left(\nabla l_i(w^{(t)})\right) + \frac{\sigma}{B}\xi\}, \end{equation}
\]</span></p>
<p>Esta es una reparametrización de la ecuación (2) en la que la tasa de aprendizaje <span class="math inline">\(\eta_t\)</span> absorbe un factor de <span class="math inline">\(C\)</span>. Esto no tiene ningún efecto sobre las garantías de privacidad, pero asegura que la norma de clipping no influya en la escala de la actualización, lo que simplifica el ajuste de los hiperparámetros. Tenga en cuenta que para preservar las garantías de DP, debemos dividir por <span class="math inline">\(C\)</span> después de la operación de clipping. El Apéndice A.1 proporciona más detalles sobre nuestra implementación de DP-SGD, incluida una descripción de nuestro enfoque de procesamiento por virtual batching para permitir el entrenamiento con grandes batch sizes.</p>
<p><strong>Privacy accounting.</strong> La garantía de privacidad de DP-SGD está determinada por tres parámetros: la desviación estándar <span class="math inline">\(\sigma\)</span>, el ratio de muestreo <span class="math inline">\(q=B/N\)</span> y el número de iteraciones de entrenamiento <span class="math inline">\(T\)</span>. En la práctica, el presupuesto de privacidad <span class="math inline">\((\epsilon, \delta)\)</span> suele ser fijo, y estos tres hiperparámetros se eligen para proporcionar el mejor rendimiento posible dentro de este presupuesto. También puede haber restricciones prácticas adicionales (por ejemplo, el presupuesto de cálculo máximo disponible). El proceso de calibración de la privacidad se realiza mediante un contador de privacidad: un algoritmo numérico que proporciona límites superiores ajustados para el presupuesto de privacidad en función de los hiperparámetros (<a href="https://arxiv.org/pdf/1607.00133.pdf">Abadi et al., 2016</a>), que a su vez puede combinarse con rutinas de optimización numérica para optimizar un hipérparametro dado el presupuesto de privacidad y los otros dos hiperparámetros.</p>
<p>En este trabajo utilizamos el método de contabilidad para DP-SGD propuesto por M<a href="https://arxiv.org/pdf/1908.10530.pdf">ironov et al.&nbsp;(2019)</a> e implementado en TensorFlow Privacy (<a href="https://github.com/tensorflow/privacy">Google, 2018</a>). Esta privacidad contable se basa en un análisis de “composición” a través de iteraciones, que nos permite liberar no solo el modelo final, sino también cada modelo intermedio obtenido durante el entrenamiento (bajo el mismo presupuesto de privacidad)</p>
<p><strong>2.3. Retos de DP-SGD</strong></p>
<p>Como describimos anteriormente, existen tres diferencias clave entre DP-SGD y SGD no privado: (1) Los gradientes por ejemplo se recortan (clipped) a una norma máxima de <span class="math inline">\(l_2\)</span> antes de promediarlos, (2) se añade ruido gaussiano a la media de los gradientes clipped, y (3) el número máximo de actualizaciones permitidas dentro del presupuesto de privacidad está limitado y depende del batch size/ruido añadido. Estas diferencias plantean una serie de retos:</p>
<p><strong>Ajuste y regularización de hiperparámetros.</strong> El ruido agregado a la estimación del gradiente en la actualización de DP-SGD (Ecuación (3)) es una barrera significativa para la optimización eficiente, y si reducimos la escala de este ruido, el número de iteraciones de entrenamiento permitidas dentro del presupuesto de privacidad disminuye. Esta restricción altera los valores óptimos de hiperparámetros clave como el batch size/learning rate, y los valores por defecto del entrenamiento no privado pueden ser muy subóptimos (<a href="https://arxiv.org/pdf/2007.14191.pdf">Papernot et al., 2021</a>). En consecuencia, DP-SGD requiere un ajuste cuidadoso de los hiperparámetros.</p>
<p>En nuestros experimentos también descubrimos que, al entrenar con DP-SGD, las mejoras en el training accuracy suelen traducirse directamente en una mejora de la generalización, sin necesidad de una fuerte regularización. Inspirados en esta observación, nuestra filosofía es que los métodos que reducen el número de iteraciones de entrenamiento necesarias para alcanzar un high training accuracy en el entrenamiento no privado probablemente mejoren el test accuracy alcanzado en el entrenamiento privado. De acuerdo con este enfoque, suele ser beneficioso eliminar los métodos de regularización explícitos.</p>
<p><strong>Bias y Varianza de la actualización DP-SGD.</strong> El estimador de gradiente utilizado por DP-SGD está biased debido al uso de gradiente clipping por ejemplo y, en general, no corresponde al gradiente de ninguna función diferenciable (<a href="https://proceedings.mlr.press/v130/song21a.html">Song et al., 2021</a>). Y lo que es más importante, la norma de clipping <span class="math inline">\(C\)</span> introduce una compensación entre sesgo y varianza (<a href="https://arxiv.org/pdf/2006.15429.pdf">Chen et al., 2020</a>; <a href="https://arxiv.org/pdf/1905.03871.pdf">Thakkar et al., 2021</a>). Esto puede verse en la actualización DP-SGD que se muestra en la ecuación (3). Cuando <span class="math inline">\(C\)</span> es muy grande, <span class="math inline">\(clip_c\)</span> es la función identidad, por lo que el gradiente privatizado es un estimador unbiased del verdadero gradiente, pero el gradiente clipped <span class="math inline">\(\frac {1}{c} clip_c(\nabla l_i(w^{(t)}))\)</span> es muy pequeño en comparación con el ruido (que es independiente de <span class="math inline">\(C\)</span>) - en general, la estimación del gradiente privatizado tiene un bias bajo y una varianza alta. Por el contrario, si <span class="math inline">\(C\)</span> es pequeño, la operación de clipping introduce un bias, pero el gradiente clipped <span class="math inline">\(\frac {1}{c} clip_c(\nabla l_i(w^{(t)}))\)</span> es mayor, y por lo tanto no es necesariamente pequeño en comparacion con el ruido- en general, la estimación del gradiente privatizado tiene un bias alto y una varianza baja. Tenga en cuenta que cuando <span class="math inline">\(C\)</span> es muy pequeño (más pequeño que la norma de gradiente más pequeña por ejemplo), reducir aún más <span class="math inline">\(C\)</span> no cambia <span class="math inline">\(\frac {1}{c} clip_c(\nabla l_i(w^{(t)}))\)</span>, lo que indica que el bias y la varianza en la actualización se aproximan a una constante a medida que <span class="math inline">\(C \longmapsto 0\)</span>. Curiosamente, trabajos anteriores han observado que amplios rangos de la norma de clipping <span class="math inline">\(C\)</span> pueden proporcionar un rendimiento casi óptimo siempre que (i) la norma de clipping sea lo suficientemente pequeña, y (ii) el learning-rate se reescalen en consecuecia <a href="https://arxiv.org/pdf/2201.12328.pdf">(Kurakin et al., 2022</a>; <a href="https://arxiv.org/pdf/2110.05679.pdf">Li et al., 2021</a>). Esto sugiere que reducir la varianza introducida por el ruido puede ser más importante que reducir el bias introducido por el clipping.</p>
<p><strong>Hacer que los modelos estándar funcionen.</strong> El entrenamiento diferencialmente privado ha obtenido recientemente resultados prometedores con arquitecturas estándar en NLP, tanto al entrenar un modelo BERT (<a href="https://arxiv.org/pdf/1810.04805.pdf">Devlin et al., 2018</a>) a partir de una inicialización aleatoria (<a href="https://arxiv.org/pdf/2108.01624.pdf">Anil et al., 2018</a>) como al afinar un gran modelo de lenguaje Transformer (<a href="https://arxiv.org/pdf/1706.03762.pdf">Vaswani et al., 2017</a>) a partir de un conjunto de parámetros preentrenados (<a href="https://arxiv.org/pdf/2110.05679.pdf">Li et al., 2021</a>; <a href="https://arxiv.org/pdf/2110.06500.pdf">Yu et al., 2021a</a>). Sin embargo, no se han obtenido resultados similares en visión por ordenador, y la bibliografía no ofrece recomendaciones claras sobre qué arquitecturas de modelos funcionan bien. Por ejemplo, analizando la investigación reciente sobre el entrenamiento privado para CIFAR-10, <a href="https://arxiv.org/pdf/2201.12328.pdf">Kurakin et al.&nbsp;(2022)</a>, <a href="https://arxiv.org/pdf/2007.14191.pdf">Papernot et al.&nbsp;(2021)</a> y <a href="https://arxiv.org/pdf/2110.06255.pdf">Dörmann et al.&nbsp;(2021)</a> utilizan variantes de modelos VGG poco profundos (<a href="https://arxiv.org/pdf/1409.1556.pdf">Simonyan y Zisserman, 2015</a>), mientras que <a href="https://arxiv.org/pdf/2011.11660.pdf">Tramèr y Boneh (2021)</a> utilizan ScatterNets (<a href="https://arxiv.org/pdf/1412.8659.pdf">Oyallon y Mallat, 2015</a>) para entrenar modelos lineales en características artesanales, logrando una impresionante precisión de prueba del 69,3 % con un presupuesto de privacidad ajustado de <span class="math inline">\((3, 10^{-5})-DP\)</span>. Por último, <a href="https://arxiv.org/pdf/2203.00324.pdf">Klause et al.&nbsp;(2022)</a> logran la precisión de prueba SOTA para 𝜀 ≤ 8 sin datos adicionales del 71,7% al entrenar una red residual superficial de 9 capas <a href="https://arxiv.org/pdf/1512.03385.pdf">(He et al., 2016</a>) bajo <span class="math inline">\((7,5, 10^{-5})-DP\)</span>.</p>
<p>La norma <span class="math inline">\(l_2\)</span> del ruido añadido en la actualización DP-SGD escala proporcionalmente a la dimensión del gradiente (el número de parámetros). Esta observación ha llevado a muchos investigadores a creer que los modelos estándar sobreparametrizados funcionarán mal con DP-SGD, y en su lugar se centran en reducir la dimensión explícita o implícita de la actualización, ya sea mediante el uso de modelos pequeños/hand-crafted features (<a href="https://arxiv.org/pdf/2011.11660.pdf">Tramèr y Boneh, 2021</a>) o mediante técnicas de reducción de la dimensionalidad (<a href="https://arxiv.org/pdf/2102.12677.pdf">Yu et al., 2021b,c</a>). Otro obstáculo clave para el uso de modelos estándar para el entrenamiento privado en visión por computador ha sido que, con el fin de proporcionar garantías ajustadas de DP, DP-SGD requiere que los gradientes evaluados en diferentes ejemplos de entrenamiento sean independientes. Esto excluye el uso de cualquier método que permita la comunicación entre ejemplos de entrenamiento, como batch normalization (<a href="https://arxiv.org/pdf/1502.03167.pdf">Ioffe y Szegedy, 2015</a>), que hasta hace poco ha sido casi omnipresente en las arquitecturas de visión estándar (<a href="https://arxiv.org/pdf/2102.06171.pdf">Brock et al., 2021b</a>; <a href="https://arxiv.org/pdf/2010.11929.pdf">Dosovitskiy et al., 2020</a>; <a href="https://arxiv.org/pdf/1512.03385.pdf">He et al., 2016</a>; <a href="https://arxiv.org/pdf/1905.11946.pdf">Tan y Le, 2019</a>; <a href="https://arxiv.org/pdf/1605.07146.pdf">Zagoruyko y Komodakis, 2016</a>).</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>