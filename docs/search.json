[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Además, me encanta leer libros de divulgación científica y ciencia ficción, así como trabajar en side projects. Me apasiona aprender constantemente y estar al día con las últimas tendencias en tecnología."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deep Learning y Privacidad",
    "section": "",
    "text": "NLP\n\n\nGPT\n\n\nPytorch\n\n\n\n\n\n\n\n\n\n\n\nJan 7, 2023\n\n\nAlexander Zelada\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nPrivacy\n\n\nPaper\n\n\nPrivacy-preserving\n\n\nDeepMind\n\n\n\n\n\n\n\n\n\n\n\nDec 9, 2022\n\n\nAlexander Zelada\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nPrivacy\n\n\nPaper\n\n\nPrivacy-preserving\n\n\nDeepMind\n\n\n\n\n\n\n\n\n\n\n\nNov 20, 2022\n\n\nAlexander Zelada\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nPrivacy\n\n\nBooks\n\n\nPrivacy-preserving\n\n\n\n\n\n\n\n\n\n\n\nOct 27, 2022\n\n\nAlexander Zelada\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nDifferential Privacy\n\n\nPrivacy-preserving\n\n\n\n\n\n\n\n\n\n\n\nMar 23, 2022\n\n\nAlexander Zelada\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nDeepLearning\n\n\nMooc\n\n\nCourse\n\n\nCoursera\n\n\n\n\n\n\n\n\n\n\n\nOct 14, 2021\n\n\nAlexander Zelada\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nDeepLearning\n\n\nBooks\n\n\nEthics\n\n\n\n\n\n\n\n\n\n\n\nJan 15, 2021\n\n\nAlexander Zelada\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nDeepLearning\n\n\nInteligenciaArtificial\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2020\n\n\nAlexander Zelada\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/01-Bienvenida/index.html",
    "href": "posts/01-Bienvenida/index.html",
    "title": "Empezando a estudiar Inteligencia Artificial",
    "section": "",
    "text": "Por otro lado, los documentos de divulgación científica se encontraban en inglés y en ese momento mi nivel de inglés era muy bajo, entonces tenía dos obstáculos: los fundamentos matemáticos y el idioma.\nAsí que lo primero que hice fue buscar “Cursos de Inteligencia Artificial“ en Google y el resultado fue de todo tipo, desde maestrías, moocs, bootcamps,repositorios de código y blogs. Algunos de ellos decían que no era necesario aprender matemáticas, otros que sí, términos como Machine Learning, Deep Learning, Computer Vision, Natural Language Processing, currsos por determinado lenguaje programación, también encontré que existían librerías como Tensorflow, Caffe, Pythorch, etc.\nY finalmente estaban los informes, encuestas y estudios sobre la situación actual y la tendencia del mercado en Inteligencia Artificial.\nSi todos estos recursos te abrumaron y acabaron con tus ganas de estudiar simplemente porque no sabías por donde empezar, verás que no fuiste el único, yo también pasé por lo mismo.\nEn ese tiempo de bloqueo me mantuve consumiendo el lado más romántico y filosófico de la inteligencia artificial mediante libros y películas de ciencia ficción, pero decidí intentar estudiar nuevamente la parte técnica, para eso me planteé eliminar los obstáculos que mencioné al inicio con los siguientes compromisos:\n\nEstudiar inglés de manera constante\nCrear un blog para llevar mis apuntes\nEstudiar los fundamentos Matemáticos\nConsumir recursos como audiobooks, ebook, papers, películas y documentales.\n\nLa razón de ser de este blog es justamente compartir mi aprendizaje contigo, hacerlo más accesible y decirte que a pesar de la gran cantidad de términos, formulas y código no tienes que ser un Andrew Ng o un Yann LeCun para comprender este mundo e ir progresando si te planteas objetivos realistas y eres constante como el T-800."
  },
  {
    "objectID": "posts/02-libros-de-inteligencia-artificial/index.html",
    "href": "posts/02-libros-de-inteligencia-artificial/index.html",
    "title": "Libros de Inteligencia Artificial para el 2021",
    "section": "",
    "text": "Los primeros días del 2021 lo he invertido en revisar los libros que quisiera leer durante el resto del año, los temas que seleccioné tienen correlación con mi plan de estudios y mi genero de literatura favorito:\nEl listado no tiene un orden de lectura. Quiero que sea una publicación viva, es decir que iré actualizándola con reseñas y más títulos; es de esperar que la gran mayoría de estos libros se encuentren en inglés (motivo para salir de mi zona de confort), pero por suerte también he agregado un link a los ya tienen una edición en español."
  },
  {
    "objectID": "posts/02-libros-de-inteligencia-artificial/index.html#libros-de-divulgación",
    "href": "posts/02-libros-de-inteligencia-artificial/index.html#libros-de-divulgación",
    "title": "Libros de Inteligencia Artificial para el 2021",
    "section": "Libros de divulgación",
    "text": "Libros de divulgación\n\nEl enemigo conoce el sistema: Manipulación de ideas, personas e influencias después de la economía de la atención\nArtifical Intelligence: A Guide for Thinking Humans\nGodel, Escher, Bach: An Eternal Golden Braid y en español se titula: Gödel, Escher, Bach: Un eterno y grácil bucle\nInvisible Women: Exposing Data Bias in a World Designed for Men y en español se titula: La mujer invisible: Descubre cómo los datos configuran un mundo hecho por y para los hombres\nThe Ethical Algorithm: The Science of Socially Aware Algorithm Design y en español se titula: El algoritmo Ético. La Ciencia Del Diseño de algoritmos socialmente responsables\nThe Overstory: A Novel y en español se titula: El clamor de los bosques\nApollo 13\nCaptivating Technology: Race, Carceral Technoscience, and Liberatory Imagination in Everyday Life\nRace After Technology: Abolitionist Tools for the New Jim Code\nWhen Breath Becomes Air y en español se titula: Recuerda que vas a morir. Vive (Los Tres Mundos)\nSupersizing the Mind: Embodiment, Action, and Cognitive Extension (Philosophy of Mind)\nThe Beginning of Infinity: Explanations That Transform the world\nOther Minds: The Octopus, the Sea, and the Deep Origins of Consciousness y en español se titula: Otras mentes. El pulpo, el mar y los origenes profundos de la consciencia\nNeural-Symbolic Cognitive Reasoning (Cognitive Technologies)\nSuperintelligence: Paths, Dangers, Strategies y en español se titula: Superinteligencia: Caminos, peligros, estrategias\nHuman + Machine: Reimagining Work in the Age of AI\nHow to Create a Mind: The Secret of Human Thought Revealed y en español se titula: Cómo crear una mente\nThe Singularity Is Near: When Humans Transcend Biology y en español se titula: La Singularidad está cerca\nThe Sentient Machine: The Coming Age of Artificial Intelligence\nThe Second Machine Age: Work, Progress, and Prosperity in a Time of Brilliant Technologies\nThe Simulation Hypothesis: An MIT Computer Scientist Shows Why AI, Quantum Physics and Eastern Mystics All Agree We Are In a Video Game\nPrivacy is Power Why and How You Should Take Back Control of Your Data\nWhat Do We Know and What Should We Do About Internet Privacy?\nPrivacy in Context: Technology, Policy, and the Integrity of Social Life\nPrivacy: A Short History\nThe Age of Surveillance Capitalism. The Fight for a Human Future at the New Frontier of Power y en español se titula: La era del capitalismo de la vigilancia: La lucha por un futuro humano frente a las nuevas fronteras del poder (Estado y Sociedad)\nT-Minus AI: Humanity’s Countdown to Artificial Intelligence and the New Pursuit of Global Power\nThinking, Fast & Slow y en español se titula: Pensar rápido, pensar despacio (Psicología)"
  },
  {
    "objectID": "posts/02-libros-de-inteligencia-artificial/index.html#libros-de-ficción",
    "href": "posts/02-libros-de-inteligencia-artificial/index.html#libros-de-ficción",
    "title": "Libros de Inteligencia Artificial para el 2021",
    "section": "Libros de ficción",
    "text": "Libros de ficción\n\nZed\nArtificial Condition: The Murderbot Diaries y en español se titula: Condición artificial: Los diarios de Matabot (Alethé)"
  },
  {
    "objectID": "posts/02-libros-de-inteligencia-artificial/index.html#libros-de-consulta",
    "href": "posts/02-libros-de-inteligencia-artificial/index.html#libros-de-consulta",
    "title": "Libros de Inteligencia Artificial para el 2021",
    "section": "Libros de consulta",
    "text": "Libros de consulta\n\nDeep Learning (Adaptive Computation and Machine Learning series)\nUnderstanding Machine Learning: From Theory to Algorithms\nIntro to Statistical Learning, with Applications in R\nThe Elements of Statistical Learning: Data Mining, Inference, and Prediction\nComputer Age Statistical Inference: Algorithms, Evidence, and Data Science\nReinforcement Learning: An Introduction (Adaptive Computation and Machine Learning series)\nAlgorithms for Data and Computation Privacy\n\nPara este post tomé como referencias a las siguientes fuentes :\n\nHAI Recommended Reading: 10 Books Worth Checking Out\nOur Privacy Opportunity course by OpenMined"
  },
  {
    "objectID": "posts/03-especializacion-deeplearning-coursera/index.html",
    "href": "posts/03-especializacion-deeplearning-coursera/index.html",
    "title": "Deep Learning Specialization - Coursera",
    "section": "",
    "text": "Este post estará dedicado al primer curso online que me hizo sentir muy feliz y orgulloso fue La especialización de Deep Learning dictada por Andrew NG (el mejor) en Coursera, y es que les voy a ser sinceros no fue nada fácil primero porque todo estaba en inglés y tenía que volver a repasar matemáticas, pero como dice Andrew NG al final de cada video:\n\n\n\nFrase celebre de Andrew Ng\n\n\nEsa frase fue crucial para poder continuar con la especialización y no es broma porque es un mes por cada curso y son cinco!! Luego para recordar tomaba apuntes de cada tema en un bloc de notas en físico de esta forma era mucho más fácil hacer los ejercicios que te dejan cada semana, hay que tener en cuenta que en más de una ocasión tendrás que repetir el video para entender el concepto (si tu motivación es aprender no te importará) abajo una foto de mis apuntes:\n\n\n\nMi cuaderno de apuntes durante el curso\n\n\nEste post no es para compartir la solución de las tareas y/o exámenes, lo que intento hacer es motivarte a estudiar aquel curso que tanto tiempo llevas postergando, siéntate abre la página web y paga con esa tarjeta de crédito la plataforma de educación online que más te guste, por último recuerda rodearte de personas que estén motivadas a aprender y no te juzguen por perseguir tus sueños.\n\n\n\nCertificado de Deep Learning Specialization"
  },
  {
    "objectID": "posts/04-por-que-la-privacidad-diferencial-es-increible/index.html",
    "href": "posts/04-por-que-la-privacidad-diferencial-es-increible/index.html",
    "title": "¿Por qué la privacidad diferencial es increíble?",
    "section": "",
    "text": "Traducción al español del post “Why differential privacy is awesome”\n¿Cómo se puede publicar los datos de las personas protegiendo su privacidad? Esta pregunta está lejos de ser nueva. Las agencias de estadística se han enfrentado a esto durante décadas. Los informáticos han elaborado varios conceptos para plasmar esta idea. Sin embargo, ninguno de ellos ha sido muy satisfactorio: se demostró que todos estos conceptos se rompían en algunas circunstancias. También eran difíciles de aplicar sin destruir la utilidad de los datos.\nTodo esto cambió en 2006, cuando cuatro investigadores introdujeron la privacidad diferencial. Este nuevo concepto adoptó un enfoque novedoso para definir la filtración de privacidad, uno que resultaría mucho más riguroso y fructífero. Entonces, ¿qué hace que la privacidad diferencial sea especial? ¿Cómo tuvo tanto éxito en los círculos académicos? ¿Por qué los gobiernos y las empresas tecnológicas comenzaron a adoptar la privacidad diferencial en la publicación de sus datos?\nEste primer artículo de introducción a la privacidad diferencial intentará responder a esa pregunta. Primero, describiremos a grandes rasgos lo que hay detrás de este concepto tan exitoso. Luego, explicaremos por qué tiene tanto éxito y por qué es mucho mejor que todos los conceptos elaborados hasta ahora"
  },
  {
    "objectID": "posts/04-por-que-la-privacidad-diferencial-es-increible/index.html#la-idea-central-detrás-de-la-privacidad-diferencial",
    "href": "posts/04-por-que-la-privacidad-diferencial-es-increible/index.html#la-idea-central-detrás-de-la-privacidad-diferencial",
    "title": "¿Por qué la privacidad diferencial es increíble?",
    "section": "La idea central detrás de la privacidad diferencial",
    "text": "La idea central detrás de la privacidad diferencial\nSuponga que tiene un proceso que toma alguna base de datos como entrada y devuelve alguna salida.\n\nPuede ser cualquier proceso. Por ejemplo:\n\nUn proceso que calcula algunas estadísticas (“dime cuántos usuarios tienen el pelo rojo”)\nUna estrategia de de-identificación (“eliminar nombres y los últimos tres dígitos de los códigos postales”)\nUn proceso de entrenamiento de machine learning (“construir un modelo para predecir a qué usuarios les gustan los gatos”)\n… Ya vas entendiendo la idea.\n\nPara hacer que un proceso sea diferencialmente privado, generalmente debes modificarlo un poco. Por lo general, se agrega algo de aleatoriedad o ruido en algunos lugares. Lo que haga exactamente y cuánto ruido se agregue depende del proceso que se esté modificando. Voy a prescindir de esa parte y simplemente diré que tu proceso ahora está haciendo una “magia” desconocida.\n\nAhora, elimina a alguien de tu base de datos y ejecuta nuevamente el proceso. Si el nuevo proceso es diferencialmente privado, entonces las dos salidas son básicamente las mismas. Esto debe ser cierto sin importar a quién se elimine y qué base de datos tenía en primer lugar.\n\nPor “básicamente lo mismo”, no me refiero a “se parece un poco”. En principio, recuerda que la magia que agregaste al proceso fue aleatoria. No siempre se obtiene el mismo resultado si se ejecuta el nuevo proceso varias veces. Entonces, ¿qué significa “básicamente lo mismo” en este contexto? Significa que puede obtener exactamente el mismo resultado de ambas bases de datos con una probabilidad similar.\n¿Qué tiene que ver esto con la privacidad? Bueno, supongamos que eres una atacante que intenta averiguar si su objetivo está en los datos originales. Con mirar el resultado final, no se puede estar 100% seguro de nada. Claro, podría haber venido de una base de datos con su objetivo en ella. Pero también podría haber venido exactamente de la misma base de datos, sin su objetivo. Ambas opciones tienen una probabilidad similar, por lo que no hay mucho que puedas decir.\nEs posible que hayas notado que esta definición no dice nada sobre cómo se ven los datos de salida. La privacidad diferencial no es una propiedad de los datos de salida. Es muy diferente, digamos, k-anonymity, una de las primeras definiciones de privacidad de datos. No puede mirar los datos de salida y determinar si satisface la privacidad diferencial. En cambio, la privacidad diferencial es una propiedad del proceso: debes saber cómo se generaron los datos para determinar si son diferencialmente privados.\nA grandes rasgos es eso. Es un poco abstracto, pero no muy complicado. Entonces, ¿por qué todo el hype? ¿Qué hace a la privacidad diferencial tan increíble en comparación con definiciones más antiguas y sencillas?"
  },
  {
    "objectID": "posts/04-por-que-la-privacidad-diferencial-es-increible/index.html#qué-hace-que-la-privacidad-diferencial-sea-especial",
    "href": "posts/04-por-que-la-privacidad-diferencial-es-increible/index.html#qué-hace-que-la-privacidad-diferencial-sea-especial",
    "title": "¿Por qué la privacidad diferencial es increíble?",
    "section": "¿Qué hace que la privacidad diferencial sea especial?",
    "text": "¿Qué hace que la privacidad diferencial sea especial?\nLos expertos en privacidad, especialmente en el mundo académico, están entusiasmados con la privacidad diferencial. Fue propuesto por primera vez por Cynthia Dwork, Frank McSherry, Kobbi Nissim y Adam Smith en 2006. Muy pronto, casi todos los investigadores que trabajaban en la anonimización comenzaron a construir algoritmos diferencialmente privados. Las empresas tecnológicas y los gobiernos lo están adoptando rápidamente. Entonces, ¿por qué todo el hype? En mi opinión existen tres razones principales.\n\n1. Ya no es necesario el modelado de ataques\nTodas las definiciones anteriores requerían algunas suposiciones sobre el atacante. Para elegir el concepto correcto, necesitas averiguar las capacidades y objetivos del atacante. ¿Cuánto conocimiento previo tienen los atacantes? ¿Qué datos auxiliares pueden usar? ¿Qué tipo de información quieren aprender?\nHacerlo en la práctica es difícil y muy propenso a errores. Responder a estas preguntas es muy complicado: en particular, es posible que no sepas exactamente lo que el atacante quiere o es capaz de hacer. Peor aún, puede haber incógnitas desconocidas: vectores de ataque que no hayas anticipado. Por esa razón, no podrías hacer declaraciones muy amplias con estas definiciones o conceptos de la vieja escuela. Tenías que hacer algunas suposiciones de las que no podías estar 100% seguro.\nPor el contrario, cuando utilizas la privacidad diferencial, obtienes dos garantías impresionantes.\nUsted protege cualquier tipo de información sobre un individuo. No importa lo que el atacante quiera hacer. Reidentificar su objetivo, saber si están en el conjunto de datos, deducir algún atributo sensible… Todas esas cosas están protegidas. Por lo tanto, no tienes que pensar en los objetivos de tu atacante. Funciona independientemente de lo que el atacante sepa sobre tus datos. Es posible que ya conozcan a algunas personas en la base de datos. Incluso podrían agregar algunos usuarios falsos a su sistema. Con privacidad diferencial, no importa. Los usuarios que el atacante aún no conoce están protegidos.\n\n\n2. Puedes cuantificar la pérdida de privacidad\nLa privacidad diferencial, como los conceptos anteriores, viene con un parámetro numérico que se puede modificar. Sin embargo, hay una gran diferencia en el significado de ese parámetro. Tomemos como ejemplo K-anonymity. Nos dice que cada registro del conjunto de datos de salida “se parece” al menorca otros “k − 1” registros. Pero, ¿el valor de “k” nos dice algo sobre el nivel de protección?\nLa respuesta es… no mucho. No existe una relación clara entre el valor de “k” y el grado de privacidad del conjunto de datos. Así que elegir “k” es muy poco preciso y no se puede justificar de manera formal. El problema es aún peor con otros conceptos de la vieja escuela.\nLa privacidad diferencial es mucho mejor. Cuando la usas, puedes cuantificar la mayor ganancia de información posible por parte del atacante. El parámetro correspondiente, llamado ε, permite hacer afirmaciones formales. Supongamos que ε = 1.1. Entonces, puedes decir: “un atacante que cree que su objetivo está en el conjunto de datos con una probabilidad del 50 % puede aumentar su nivel de certeza hasta un 75 % como máximo”. Elegir el valor exacto de ε no es fácil, pero al menos se puede interpretar de manera formal.\n¿Y recuerdas el punto anterior sobre el modelado de ataques? Significa que puedes cambiar esta declaración de muchas maneras. Puede reemplazar “su objetivo es el conjunto de datos” por cualquier cosa sobre un individuo. Y puede agregar “no importa lo que el atacante sepa” si desea ser más preciso. En resumen, todo esto hace que la privacidad diferencial sea mucho más fuerte que todas las definiciones anteriores.\n\n\n3. Puedes elaborar múltiples mecanismos\nSupongamos que tienes algunos datos. Quieres compartirlos con Alex y con Brinn, de forma anónima. Confías en Alex y en Brinn por igual, así que utilizas la misma definición de privacidad para ambos. No les interesan los mismos aspectos de los datos, así que les das dos versiones diferentes de tus datos. Ambas versiones son “anónimas”, según la definición que hayas elegido.\n¿Qué ocurre si Alex y Brinn deciden conspirar y comparar los datos que les has dado? ¿La unión de las dos versiones anonimizadas seguirá siendo anónima? Resulta que para la mayoría de las definiciones de privacidad, este no es el caso. Si juntas dos versiones k-anónimas de los mismos datos, el resultado no será k-anónimo. Así que si Alex y Brinn colaboran, podrían ser capaces de reidentificar a los usuarios por su cuenta… ¡O incluso reconstruir todos los datos originales! Eso no es una buena noticia.\nCon la privacidad diferencial, puedes evitar este modo de fallo. Supongamos que das datos con privacidad diferencial a Alex y Brinn. Cada vez, usaste un parámetro de ε. Entonces, si conspiran, los datos resultantes siguen estando protegidos por la privacidad diferencial. El nivel de privacidad es ahora más débil: el parámetro pasa a ser 2ε. Así que siguen ganando algo de información, pero ahora se puede cuantificar cuánta. Esta propiedad se llama composición.\nEste escenario suena un poco improbable, pero la composición es super útil en la práctica. Las organizaciones suelen querer hacer muchas cosas con los datos. Publicar estadísticas, liberar una versión anónima, entrenar algoritmos de aprendizaje automático… La composición es una forma de mantener el control del nivel de riesgo a medida que aparecen nuevos casos de uso y los procesos evolucionan."
  },
  {
    "objectID": "posts/04-por-que-la-privacidad-diferencial-es-increible/index.html#conclusión",
    "href": "posts/04-por-que-la-privacidad-diferencial-es-increible/index.html#conclusión",
    "title": "¿Por qué la privacidad diferencial es increíble?",
    "section": "Conclusión",
    "text": "Conclusión\nEspero que la intuición básica de la privacidad diferencial esté cristalina. Si recuerdas una sola cosa, que sea este resumen de una línea: la incertidumbre en el proceso significa incertidumbre para el atacante, lo que significa mejor privacidad.\nTambién espero que ahora te preguntes cómo funciona realmente. ¿Qué se esconde detrás de esta magia que hace que todo sea seguro y privado? ¿Por qué la privacidad diferencial tiene todas las increíbles propiedades que he mencionado? Este es el tema exacto del siguiente artículo de esta serie, que lo explica con más detalle sin dejar de lado las matemáticas pesadas.\n\nEl autor original de este post es Damien Desfontaine, en Twitter como @TedOnPrivacy, y el motivo principal que me llevó a traducir la publicación original al español fue el aprendizaje y aportar contenido a la comunidad hispana de Privacidad Diferencial."
  },
  {
    "objectID": "posts/05-10-Books-on-Privacy-and-AI/index.html",
    "href": "posts/05-10-Books-on-Privacy-and-AI/index.html",
    "title": "10 Books on Privacy and AI",
    "section": "",
    "text": "That’s why I believe that everyone should educate themselves on the concept of privacy and privacy-enhancing technologies.\nI think that these books are very useful to understand the concept of privacy and the privacy enhancing technologies like Differential Privacy, Homomorphic Encryption Secure, Federate Learning.\n\n\n\n1. Privacy-Preserving Machine Learning\nIs a comprehensive guide to avoiding data breaches in your machine learning projects. You’ll get to grips with modern privacy-enhancing techniques such as differential privacy, compressive privacy, and synthetic data generation\nLink: https://www.manning.com/books/privacy-preserving-machine-learning\nAutor: J. Morris Chang, Di Zhuang, and G. Dumindu Samaraweera\n\n\n\n2. Data Privacy, A runbook for engineers\nData Privacy teaches you to design, develop, and measure the effectiveness of privacy programs. You’ll learn from author Nishant Bhajaria, an industry-renowned expert who has overseen privacy at Google, Netflix, and Uber\nLink: https://www.manning.com/books/data-privacy\nAutor: Nishant Bhajaria\n\n\n\n3. Privacy is Power: Why and How You Should Take Back Control of Your Data\nShort, terrifying, practical: Privacy is Power highlights the implications of our laid-back attitude to data and sets out how we can take back control.\nLink: http://bitly.ws/vQQi\nAutor: Carissa Véliz\n\n\n\n4. The Fight for Privacy: Protecting Dignity, Identity, and Love in the Digital Age\nYet there is a solution to our toxic relationship with technology and privacy: fighting for intimate privacy as a civil right.\nLink: https://www.amazon.com/Fight-Privacy-Protecting-Dignity-Identity/dp/0393882314\nAutor: Danielle Keats Citron\n\n\n\n5. Why Privacy Matters\nPrivacy matters because good privacy rules can promote the essential human values of identity, power, freedom, and trust. If we want to preserve our commitments to these precious yet fragile values, we will need privacy rules\nLink: http://bitly.ws/vQQ9\nAutor: Neil Richards\n\n\n\n6. What Do We Know and What Should We Do About Internet Privacy?\nThe author then proposes what we should do about the problems surrounding internet privacy, such as significant changes in government policy, a reversal of the current ‘war’ on encryption, being brave enough to take on the internet giants, and challenging the idea that ‘real names’ would improve the discourse on social networks.\nLink: http://bitly.ws/vQPZ\nAutor: Paul Bernal\n\n\n\n7. Privacy in Context: Technology, Policy, and the Integrity of Social Life\nThis book claims that what people really care about when they complain and protest that privacy has been violated is not the act of sharing information itself―most people understand that this is crucial to social life ―but the inappropriate, improper sharing of information.\nLink: http://bitly.ws/vQPL\nAutor: Helen Nissenbaum\n\n\n\n8. Privacy: A Short History\nPrivacy: A Short History provides a vital historical account of an increasingly stressed sphere of human interaction. At a time when the death of privacy is widely proclaimed, distinguished historian, David Vincent, describes the evolution of the concept and practice of privacy from the Middle Ages to the present controversy over digital communication and state surveillance provoked by the revelations of Edward Snowden.\nLink: http://bitly.ws/vQPC\nAutor: David Vincent\n\n\n\n9. Introduction to Privacy Enhancing Technologies\nThis textbook provides a unique lens through which the myriad of existing Privacy Enhancing Technologies (PETs) can be easily comprehended and appreciated. It answers key privacy-centered questions with clear and detailed explanations.\nLink: https://link.springer.com/book/10.1007/978-3-030-81043-6\nAutor: Carlisle Adams\n\n\n\n10. The Algorithmic Foundations of Differential Privacy\nAfter motivating and discussing the meaning of differential privacy, the preponderance of this monograph is devoted to fundamental techniques for achieving differential privacy, and application of these techniques in creative combinations, using the query-release problem as an ongoing example\nLink: http://bitly.ws/vQPb\nAutor: Cynthia Dwork and Aaron Roth"
  },
  {
    "objectID": "posts/06-Unlocking-High-Accuracy-Differentially-Private-Image-Classification-through-Scale - Parte1/index.html",
    "href": "posts/06-Unlocking-High-Accuracy-Differentially-Private-Image-Classification-through-Scale - Parte1/index.html",
    "title": "Unlocking High-Accuracy Differentially Private Image Classification through Scale - Parte 1",
    "section": "",
    "text": "(DP-SGD), el método de entrenamiento de DP más popular para el aprendizaje profundo, realiza esta protección mediante la inyección de ruido durante el entrenamiento.\nVoy a trabajar en la traducción del paper “Unlocking High-Accuracy Differentially Private Image Classification through Scale”, este documento consta de 6 secciones y cada post será una de ellas. Como parte de mi aprendizaje considero importante generar información en mi lengua materna."
  },
  {
    "objectID": "posts/06-Unlocking-High-Accuracy-Differentially-Private-Image-Classification-through-Scale - Parte1/index.html#introducción",
    "href": "posts/06-Unlocking-High-Accuracy-Differentially-Private-Image-Classification-through-Scale - Parte1/index.html#introducción",
    "title": "Unlocking High-Accuracy Differentially Private Image Classification through Scale - Parte 1",
    "section": "1.- Introducción",
    "text": "1.- Introducción\nLos modelos de Machine Learning entrenados de manera estándar pueden ser atacados por un adversario que busca revelar los datos con los que se entrenó el modelo. Por ejemplo, Carlini et al. (2021b) demuestra que los adversarios pueden generar y detectar secuencias de texto de un conjunto de entrenamiento de un “large transformer language model”. Mientras, Balle et al. (2022) Demostró que poderosos adversarios pueden reconstruir imágenes en el conjunto de entrenamiento de un clasificador entrenado en CIFAR-10. Junto a otros resultados (Carlini et al., 2021a; Choquette-Choo et al., 2021; Liu et al., 2021), estos estudios demuestran que los modelos entrenados con datasets confidenciales presentan un riesgo significativo de privacidad.\nLa Privacidad Diferencial es una técnica para mitigar ataques de privacidad dirigidos a filtrar ejemplos individuales de entrenamiento, y ya ha sido adoptada en la práctica por una variedad de organizaciones públicas y privadas (Abowd, 2018; Apple Differential Privacy Team, 2017; Bird, 2020; Erlingsson, 2014; McMahan and Thakurta, 2022; Nayak, 2020). Un Algoritmo diferencialmente privado es un algoritmo aleatorio que proporciona una garantía formal de que cualquier ejemplo individual en el conjunto de entrenamiento solo puede influir en la distribución de salida del algoritmo en una cantidad pequeña y preestablecida. Esta garantía de privacidad, es denominada \\((\\epsilon, \\delta)-DP\\), está definida por dos parámetros \\((\\epsilon, \\delta)\\) a los cuales nos referimos como el presupuesto de privacidad o “privacy budget” en inglés. Cuantos más pequeños sean estos dos parámetros; más cercanas serán las distribuciones de salida entre conjuntos de entrenamiento que difiere en un solo ejemplo, y por lo tanto, más difícil será para un adversario inferir si un solo ejemplo o single data point fue incluido durante el entrenamiento.\nEl método más popular para el entrenamiento de redes neuronales con DP es Differentially Private Stochastic Gradient Descent (DP-SGD) (Abadi et al., 2016). DP-SGD sustituye la estimación habitual de gradiente mini-batch por una versión privatizada, en el que el gradiente de cada ejemplo de entrenamiento se recorta a una norma máxima. Además, el ruido gaussiano proporcional a la norma de recorte se agrega a la suma de los gradientes recortados, lo que es suficiente para enmascarar la contribución de cualquier ejemplo individual a la suma. Cada evaluación de un gradiente de mini-batch (privatizado) incurre en un coste de privacidad, y se utiliza un contador de privacidad (Abadi et al., 2016; Mironov et al., 2019) para rastrear el presupuesto total de privacidad gastado a lo largo del entrenamiento. Estos parámetros aumentan con cada mini-batch visto durante el entrenamiento y disminuyen con la escala del ruido agregado, lo que limita la cantidad de iteraciones de entrenamiento que podemos realizar con un presupuesto de privacidad fijo mientras se mantiene bajo control la variación en la estimación del gradiente.\n\n\n\n\nFigura 1 | (a) Cuando entrenamos en CIFAR-10 sin datos adicionales, mejoramos los resultados publicados previamente bajos \\(( \\epsilon, 10^{-5} )-DP\\) siempre que \\(\\epsilon ≥ 3\\). En \\(\\epsilon = 8\\), mejoramos el SOTA anterior de Klause et al. (2022) en un 9,7%. Tenga en cuenta que informamos la media y el error estándar en 5 ejecuciones independientes. (b) Al ajustar un NFNet-F3 preentrenado (Brock et al., 2021b) en ImageNet bajo \\((8, 8 · 10^{-7})-DP\\), logramos una precisión del 86,7 % entre el top-1, solo un 4,3 % por debajo la actual SOTA no privada del 91,0% (Yu et al., 2022). También obtenemos una precisión del 83,8 % en el top-1 con una garantía mucho más estricta de \\((0,5, 8 · 10^{-7})-DP\\), que supera el rendimiento de muchos modelos no privados populares (p. ej., ResNet-50).\n\nEl entrenamiento con DP-SGD implica un delicado acto de equilibrio entre diferentes hiperparámetros como la cantidad de ruido agregado, el batch size, el número iteraciones de entrenamiento, para alcanzar el rendimiento óptimo dentro de un presupuesto de privacidad específico. En particular, el ruido agregado al gradiente es una barrera importante para la optimización, lo que generalmente resulta en una degradación significativa en el rendimiento en comparación con el entrenamiento estándar no privado o “non-private” en inglés (Dörmann et al., 2021; Klause et al., 2022; Kurakin et al., 2022). Además, varios autores han postulado que los modelos altamente sobre parametrizados, que funcionan bien en entornos no privados, no funcionan bien cuando se usan con DP-SGD, porque la norma del ruido agregado aumenta con la dimensión de la gradiente (Kurakin et al., 2022; Shen et al., 2021; Tramèr and Boneh, 2021; Yu et al., 2021b), lo que lleva a una “maldición de la dimensionalidad”. En consecuencia, muchos trabajos se han centrado en desarrollar arquitecturas especializadas para formación privada (Papernot et al., 2021; Tramèr and Boneh, 2021), o en reducir la dimensionalidad del modelo durante el entrenamiento (Golatkar et al., 2022; Yu et al., 2021b; Zhang et al., 2021; Zhou et al., 2021).\nPor el contrario, mostramos que las arquitecturas estándar sobre parametrizadas, que logran un rendimiento cercano al estado del arte en el entrenamiento no privado, también pueden funcionar muy bien cuando se entrenan con DP-SGD si se ajustan correctamente. Para lograr esto, presentamos una serie de técnicas que ayudan a la convergencia y aseguran la capacidad de entrenamiento en la inicialización, y exploramos los beneficios de usar modelos previamente entrenados. Nuestras principales contribuciones se enumeran a continuación:\n\nDescribimos un conjunto de técnicas simples que, cuando se combinan, mejoran significativamente el rendimiento DP-SGD. Primero, revisamos las ideas que anteriormente se identificaron como útiles para el entrenamiento privado, incluido el uso de large batch size (McMahan et al., 2018b) y el reemplazo de las capas de normalización de lotes con alternativas que garantizan una buena propagación de la señal en la inicialización (van der Maaten and Hannun, 2020). Además, proponemos modificaciones adicionales que mejoran la tasa de convergencia de DP-SGD y que no se han utilizado previamente para el entrenamiento privado. Específicamente, sugerimos usar la estandarización de peso en capas convolucionales (Qiao et al., 2019) aprovechando los beneficios de data augmentations promediando gradientes, por ejemplo, en múltiples augmentations de la misma imagen antes de la operación de recorte (Hoffer et al., 2019) y aplicar técnicas de promedio de parámetros (Polyak and Juditsky, 1992).\nAl aplicar las técnicas anteriores, mejoramos significativamente el rendimiento de DP-SGD al entrenar modelos sobreparametrizados inicializados aleatoriamente.\n\nEntrenando Wide-ResNets (Zagoruyko and Komodakis,2016) en CIFAR-10 sin datos adicionales, logramos un nuevo SOTA de 81.4% bajo \\((8, 10^{-5})-DP\\).\nEsta es una mejora sustancial con respeto al SOTA anterior de 71,7% logrado con \\((7.5, 10^{-5})-DP\\) (Klause et al.,2022).\nComo se muestra en la Figura 1(a), logramos resultados SOTA en esta tarea en un rango de valores de \\(\\epsilon\\) entre 3 y 8. También logramos una nueva precisión SOTA top-1 en ImageNet del 32,4 % en \\((8, 8*10^{-7})-DP\\) al entrenar un ResNet-50 sin normalizador (NF-ResNet-50) (Brock et al., 2021a; He et al., 2016).\n\n\n\nTabla 1 | Un resumen de los mejores resultados proporcionados en este documento cuando se entrena con DP-SGD. Todos los números en negrita son SOTA. Para los experimentos CIFAR-10 y CIFAR-100, informamos la precisión media en 5 ejecuciones independientes. Todos los experimentos en CIFAR usan Wide-ResNets con normalización de grupo, mientras que los experimentos ImageNet y Places-365 usan NF-ResNets o NFNets. Consulte las secciones correspondientes para obtener más detalles.\n\n\n\n\n\nMostramos que el entrenamiento previo no privado en datos públicos/no confidenciales, seguido de un fine-tuning con DP-SGD en el conjunto de datos privados, produce beneficios de rendimiento notables en los puntos de referencia de clasificación de imágenes. Por ejemplo, cuando hacemos fine-tuning de forma privada un NF-ResNet-200 entrenado previamente en JFT-300M (Sun et al., 2017), logramos una precisión del 81,3% en el top-1 en ImageNet por debajo de \\((8, 8*10^{-7})-DP\\). Observamos mejoras adicionales en el rendimiento al aumentar tanto el tamaño del modelo como el tamaño del conjunto de datos de preentrenamiento, logrando una precisión del 86,7% en el top-1 en \\((8, 8*10^{-7})-DP\\) con un NFNet-F3 (Brock et al., 2021b) entrenado previamente en JFT-4B. Esta red también obtiene una precisión del 83,8% en el top-1 con un presupuesto de privacidad mucho más ajustado de \\((0.5, 8*10^{-7})-DP\\). A modo de comparación, el fine-tuning de la misma red pre entrenada en ImageNet sin privacidad alcanza el 88.5%.\nBrindamos información novedosa sobre cómo los hiperparámetros óptimos se relacionan entre sí cuando se entra con DP. Observamos empíricamente que hay un presupuesto óptimo de iteraciones de entrenamiento dado un batch-size fijo, los batch sizes más grandes mejoran el accuracy de la validación, pero requieren más épocas de entrenamiento después de que el batch size supera un cierto umbral, y la elección óptima del learning-rate para DP-SGD es proporcional al batch size cuando el batch size es pequeño, pero constante para batch sizes más grande, similar al entrenamiento no privado.\n\nResumimos nuestros resultados clave en la Tabla 1, con nuestros resultados SOTA mostrados en negrita. Hacemos hincapié en que todos nuestros resultados utilizan arquitecturas de visión estándar que han demostrado funcionar bien para el entrenamiento no privado. Creemos que estos resultados son un paso significativo hacia una clasificación de imágenes privada diferencialmente útil en la práctica.\n\nEsquema del paper.\n\nBrindamos una breve introducción a la privacidad diferencial y DP-SGD en la sección 2, donde también analizamos los desafíos que surgen al aplicar DP-SGD a deep networks.\nEn la sección 3, describimos una variedad de técnicas que mejoran el rendimiento de las redes entrenadas con DP-SGD, logrando el rendimiento de SOTA en CIFAR-10 e ImageNet cuando se entrena sin datos adicionales.\nEn la sección 4, mostramos que el fine-tuning privado de modelos fuertes previamente entrenados mejora drásticamente el rendimiento de la clasificación de imágenes privadas.\nFinalmente, proporcionamos información adicional sobre cómo los hiperparámetros de DP-SGD influyen en el rendimiento en la sección 5.\n\n\n\nReproductibilidad.\nPara ayudar a los investigadores a reproducir y verificar nuestros resultados, lanzamos la implementación de DP-SGD utilizado en nuestros experimentos en https://github.com/deepmind/jax_privacy. También proporcionamos los scripts de configuración y los checkpoints pre entrenados necesarios para reproducir todos nuestros resultados en CIFAR-10 y CIFAR-100, así como nuestros resultados en ImageNet sin datos adicionales. Proporcionamos más detalles sobre nuestra implementación de DP-SGD en el Apéndice A, junto con una descripción de los pasos que llevamos a cabo para auditar su corrección."
  },
  {
    "objectID": "posts/07-Unlocking-High-Accuracy-Differentially-Private-Image-Classification-through-Scale - Parte2/index.html",
    "href": "posts/07-Unlocking-High-Accuracy-Differentially-Private-Image-Classification-through-Scale - Parte2/index.html",
    "title": "Unlocking High-Accuracy Differentially Private Image Classification through Scale - Parte 2",
    "section": "",
    "text": "(DP-SGD), el método de entrenamiento de DP más popular para el aprendizaje profundo, realiza esta protección mediante la inyección de ruido durante el entrenamiento.\nVoy a trabajar en la traducción del paper “Unlocking High-Accuracy Differentially Private Image Classification through Scale”, este documento consta de 6 secciones y cada post será una de ellas. Como parte de mi aprendizaje considero importante generar información en mi lengua materna."
  },
  {
    "objectID": "posts/07-Unlocking-High-Accuracy-Differentially-Private-Image-Classification-through-Scale - Parte2/index.html#background",
    "href": "posts/07-Unlocking-High-Accuracy-Differentially-Private-Image-Classification-through-Scale - Parte2/index.html#background",
    "title": "Unlocking High-Accuracy Differentially Private Image Classification through Scale - Parte 2",
    "section": "2.- Background",
    "text": "2.- Background\n2.1. Privacidad Diferencial DP\nLa privacidad diferencial es una garantía de privacidad formal que se aplica a los algoritmos de análisis de datos aleatorios. Por construcción, los algoritmos diferencialmente privados evitan que un adversario que observa el resultado de un cómputo deduzca cualquier propiedad relacionada con data points individuales en los datos de entrada utilizados durante el cómputo.\nLa fuerza de esta garantía está controlada por dos parámetros: \\(\\epsilon>0\\) y \\(\\delta \\in [0,1]\\). En términos generales, \\(\\epsilon\\) limita la relación logarítmica de verosimilitud de cualquier resultado particular que se puede obtener al ejecutar el algoritmo en dos conjuntos de datos que difieren en un solo punto de datos, y \\(\\delta\\) es una pequeña probabilidad que limita la aparición de resultados poco frecuentes que violan este límite. La garantía de privacidad se fortalece a medida que ambos parámetros se reducen. Una regla general estándar establece que, para obtener una privacidad significativa, \\(\\epsilon\\) debe ser una constante pequeña mientras que \\(\\delta\\) debe ser menor que \\(1/N\\), donde \\(N\\) es el tamaño del conjunto de datos de entrada. Más formalmente, tenemos lo siguiente.\nDefinición 2.1 (Differential Privacy (Dwork et al., 2006)). Sea \\(A:D \\longmapsto S\\) un algoritmo aleatorio, y sea : \\(\\epsilon>0\\) y \\(\\delta \\in [0,1]\\). Decimos que \\(A\\) es \\((\\epsilon, \\delta)-DP\\) si para dos conjuntos de datos vecinos cualesquiera \\(D, D{'} \\in D\\) difieren en un solo elemento, tenemos que:\n\\[\n\\begin{equation} \\lor S \\subset S, P[A(D)\\in S] \\leq exp(\\epsilon)P[A(D{'})\\in S] + \\delta  \\end{equation}\n\\]\nLa protección de la privacidad que brinda DP se mantiene bajo un modelo de amenaza extremadamente fuerte: las inferencias sobre las personas están protegidas incluso frente a un adversario que tiene pleno conocimiento del algoritmo DP, poder computacional ilimitado y arbitrary side knowledge sobre los datos de entrada. Además, DP satisface una serie de propiedades atractivas desde el punto de vista del diseño de algoritmos, incluida la conservación bajo procesamiento posterior y una degradación suave con múltiple accesos a los mismos datos. Estas propiedades se explotan en la construcción de algoritmos DP complejos basados en la combinación de pequeños bloques de construcción que inyectan ruido cuidadosamente calibrado en las operaciones que acceden a los datos. La magnitud del ruido requerido para satisfacer la garantía de privacidad aumenta con la fuerza de los parámetros de privacidad, lo que lleva a una compensación inevitable entre utilidad y privacidad, como lo ilustra la Ley Fundamental de Recuperación de Información (Dwork and Roth, 2014).\nJuntos, la solidez de la garantía formal que brinda y la variedad de herramientas disponibles para la construcción de algoritmos de DP han llevado a la creciente adopción de DP como un estándar de oro para el aprendizaje automático que preserva la privacidad. Para problemas de convex learning, existe una variedad de métodos para obtener algoritmos diferencialmente privados, incluida la perturbación de salida (Chaudhuri et al., 2011; Wu et al., 2017), la perturbación objetiva (Chaudhuri et al., 2011; Kifer et al., 2012) y la perturbación de gradiente (Bassily et al., 2014; Song et al., 2013) .\nLa naturaleza de los problemas convexos permite el análisis formal de la utilidad de privacidad que ofrecen estos algoritmos, y en la actualidad existen grandes clases de problemas para los cuales se conocen algoritmos que logran (casi) equilibrios óptimos de utilidad de privacidad (Asi et al., 2021; Bassily et al., 2014; Feldman et al., 2020; Song et al., 2021; Talwar et al., 2015). Para los problemas de aprendizaje no convexos, la gama de algoritmos disponibles es más limitada y las ventajas y desventajas de la privacidad y la utilidad son más difíciles de analizar teóricamente. No obstante, para tales problemas existen dos familias de algoritmos que han demostrado lograr compensaciones razonables de privacidad-utilidad-cómputo en la práctica; perturbación de gradiente aplicada a optimizadores estándar como SGD (Abadi et al., 2016), y agregación privada de teacher ensembles (Papernot et al., 2018). En este trabajo nos centramos en el primero, que es el más utilizado.\n2.2. Descenso de gradiente estocástico diferencialmente privado (DP-SGD)\nEn este trabajo, suponemos que el algoritmo diferencialmente privado A, es un algoritmo de aprendizaje que mapea un conjunto de datos de entrenamiento: \\(D = \\lbrace(x_i, y_i)\\rbrace_{1\\leq i\\leq N}\\) a un vector de parámetros de la red neuronal aprendida \\(w \\in S = R^{p}\\). Sea \\(\\mathcal{L}(w,x,y)\\) el objetivo de aprendizaje (por ejemplo, la pérdida de entropía cruzada), dados los parámetros del modelo \\(w\\), la entrada ejemplo \\(x\\) y la etiqueta \\(y\\). Por comodidad, utilizamos la notación abreviada \\(l_i(w) = \\mathcal{L}(w,x_i,y_i)\\).\nEn la configuración non-private, una actualización de parámetros utilizando el Descenso de Gradiente Estocástico (SGD) en la iteración \\(t\\) extrae \\(B\\) ejemplos al azar del conjuntos de datos, y realiza una actualización de la forma:\n\\[\nw^{(t+1)}= w^{(t)} - \\eta_{t}\\frac{1}{B} \\sum_{i \\in \\beta_t}\\nabla l_i(w^{(t)}),\n\\]\nDonde \\(\\eta_t\\) es el step-size para una actualización \\(t^{th}\\), \\(\\nabla\\) denota el operador de gradiente, y \\(B_t\\) representa el conjunto de ejemplos muestreados en la iteración \\(t\\) con \\(|B_t| = B\\). Para que este algoritmo sea diferencialmente privado, aplicamos las siguientes modificaciones. En primer lugar, el gradiente de cada ejemplo del mini-batch es clipped a una norma máxima \\(C\\), y en segundo lugar, se añade ruido gaussiano con desviación estándar proporcional a \\(C\\) se añade a la media de los grandientes clipped.\nSea\n\\[\nclip_x:v\\in R^{p}\\longmapsto min\\lbrace{1,\\frac{c}{||v||_2}\\rbrace}.v\\in R^{p}\n\\]\ndenote la función de clipping que reescala su entrada para que la salida tenga una norma máxima \\(l_2\\) de \\(C\\). El nuevo step de actualización es:\n\\[\n\\begin{equation}  w^{(t+1)}= w^{(t)} -\\eta_{t} \\{\\frac{1}{B}\\sum_{i \\in \\beta_t}clip_c\\left(\\nabla l_i(w^{(t)})\\right) +  \\frac{\\sigma C}{B}\\xi\\}, \\end{equation}\n\\]\nDonde \\(\\xi \\sim N(0,I_p)\\) es una variable aleatoria gaussiana estándar de \\(p\\) dimensiones y \\(\\sigma\\) especifica la desviación estándar del ruido añadido. El algoritmo resultante se llama Differentially Private-Stochastic Gradient Descent (DP-SGD) (Abadi et al., 2016). Intuitivamente, realizar una actualización del modelo utilizando la ecuación proporciona privacidad diferencial porque la adición de ruido gaussiano con desviación estándar proporcional a \\(C\\) es suficiente para enmascarar la contribución de cualquier ejemplo individual cuyo gradiente recortado tenga una norma menor o igual a \\(C\\). Aunque utilizamos el SGD privatizado como nuestro optimizador a lo largo de este trabajo, también se puede utilizar un método de privatización similar en combinación con otros algoritmos de optimización de primer orden, como SGD con momentum o Adam (McMahan et al., 2018a).\nA lo largo de este trabajo utilizamos una versión modificada de DP-SGD en la que el gradiente privatizado está normalizado por \\(C\\):\n\\[\n\\begin{equation} w^{(t+1)}= w^{(t)} -\\eta_{t} \\{\\frac{1}{B}\\sum_{i \\in \\beta_t}\\frac{1}{C} clip_c\\left(\\nabla l_i(w^{(t)})\\right) + \\frac{\\sigma}{B}\\xi\\}, \\end{equation}\n\\]\nEsta es una reparametrización de la ecuación (2) en la que la tasa de aprendizaje \\(\\eta_t\\) absorbe un factor de \\(C\\). Esto no tiene ningún efecto sobre las garantías de privacidad, pero asegura que la norma de clipping no influya en la escala de la actualización, lo que simplifica el ajuste de los hiperparámetros. Tenga en cuenta que para preservar las garantías de DP, debemos dividir por \\(C\\) después de la operación de clipping. El Apéndice A.1 proporciona más detalles sobre nuestra implementación de DP-SGD, incluida una descripción de nuestro enfoque de procesamiento por virtual batching para permitir el entrenamiento con grandes batch sizes.\nPrivacy accounting. La garantía de privacidad de DP-SGD está determinada por tres parámetros: la desviación estándar \\(\\sigma\\), el ratio de muestreo \\(q=B/N\\) y el número de iteraciones de entrenamiento \\(T\\). En la práctica, el presupuesto de privacidad \\((\\epsilon, \\delta)\\) suele ser fijo, y estos tres hiperparámetros se eligen para proporcionar el mejor rendimiento posible dentro de este presupuesto. También puede haber restricciones prácticas adicionales (por ejemplo, el presupuesto de cálculo máximo disponible). El proceso de calibración de la privacidad se realiza mediante un contador de privacidad: un algoritmo numérico que proporciona límites superiores ajustados para el presupuesto de privacidad en función de los hiperparámetros (Abadi et al., 2016), que a su vez puede combinarse con rutinas de optimización numérica para optimizar un hipérparametro dado el presupuesto de privacidad y los otros dos hiperparámetros.\nEn este trabajo utilizamos el método de contabilidad para DP-SGD propuesto por Mironov et al. (2019) e implementado en TensorFlow Privacy (Google, 2018). Esta privacidad contable se basa en un análisis de “composición” a través de iteraciones, que nos permite liberar no solo el modelo final, sino también cada modelo intermedio obtenido durante el entrenamiento (bajo el mismo presupuesto de privacidad)\n2.3. Retos de DP-SGD\nComo describimos anteriormente, existen tres diferencias clave entre DP-SGD y SGD no privado: (1) Los gradientes por ejemplo se recortan (clipped) a una norma máxima de \\(l_2\\) antes de promediarlos, (2) se añade ruido gaussiano a la media de los gradientes clipped, y (3) el número máximo de actualizaciones permitidas dentro del presupuesto de privacidad está limitado y depende del batch size/ruido añadido. Estas diferencias plantean una serie de retos:\nAjuste y regularización de hiperparámetros. El ruido agregado a la estimación del gradiente en la actualización de DP-SGD (Ecuación (3)) es una barrera significativa para la optimización eficiente, y si reducimos la escala de este ruido, el número de iteraciones de entrenamiento permitidas dentro del presupuesto de privacidad disminuye. Esta restricción altera los valores óptimos de hiperparámetros clave como el batch size/learning rate, y los valores por defecto del entrenamiento no privado pueden ser muy subóptimos (Papernot et al., 2021). En consecuencia, DP-SGD requiere un ajuste cuidadoso de los hiperparámetros.\nEn nuestros experimentos también descubrimos que, al entrenar con DP-SGD, las mejoras en el training accuracy suelen traducirse directamente en una mejora de la generalización, sin necesidad de una fuerte regularización. Inspirados en esta observación, nuestra filosofía es que los métodos que reducen el número de iteraciones de entrenamiento necesarias para alcanzar un high training accuracy en el entrenamiento no privado probablemente mejoren el test accuracy alcanzado en el entrenamiento privado. De acuerdo con este enfoque, suele ser beneficioso eliminar los métodos de regularización explícitos.\nBias y Varianza de la actualización DP-SGD. El estimador de gradiente utilizado por DP-SGD está biased debido al uso de gradiente clipping por ejemplo y, en general, no corresponde al gradiente de ninguna función diferenciable (Song et al., 2021). Y lo que es más importante, la norma de clipping \\(C\\) introduce una compensación entre sesgo y varianza (Chen et al., 2020; Thakkar et al., 2021). Esto puede verse en la actualización DP-SGD que se muestra en la ecuación (3). Cuando \\(C\\) es muy grande, \\(clip_c\\) es la función identidad, por lo que el gradiente privatizado es un estimador unbiased del verdadero gradiente, pero el gradiente clipped \\(\\frac {1}{c} clip_c(\\nabla l_i(w^{(t)}))\\) es muy pequeño en comparación con el ruido (que es independiente de \\(C\\)) - en general, la estimación del gradiente privatizado tiene un bias bajo y una varianza alta. Por el contrario, si \\(C\\) es pequeño, la operación de clipping introduce un bias, pero el gradiente clipped \\(\\frac {1}{c} clip_c(\\nabla l_i(w^{(t)}))\\) es mayor, y por lo tanto no es necesariamente pequeño en comparacion con el ruido- en general, la estimación del gradiente privatizado tiene un bias alto y una varianza baja. Tenga en cuenta que cuando \\(C\\) es muy pequeño (más pequeño que la norma de gradiente más pequeña por ejemplo), reducir aún más \\(C\\) no cambia \\(\\frac {1}{c} clip_c(\\nabla l_i(w^{(t)}))\\), lo que indica que el bias y la varianza en la actualización se aproximan a una constante a medida que \\(C \\longmapsto 0\\). Curiosamente, trabajos anteriores han observado que amplios rangos de la norma de clipping \\(C\\) pueden proporcionar un rendimiento casi óptimo siempre que (i) la norma de clipping sea lo suficientemente pequeña, y (ii) el learning-rate se reescalen en consecuecia (Kurakin et al., 2022; Li et al., 2021). Esto sugiere que reducir la varianza introducida por el ruido puede ser más importante que reducir el bias introducido por el clipping.\nHacer que los modelos estándar funcionen. El entrenamiento diferencialmente privado ha obtenido recientemente resultados prometedores con arquitecturas estándar en NLP, tanto al entrenar un modelo BERT (Devlin et al., 2018) a partir de una inicialización aleatoria (Anil et al., 2018) como al afinar un gran modelo de lenguaje Transformer (Vaswani et al., 2017) a partir de un conjunto de parámetros preentrenados (Li et al., 2021; Yu et al., 2021a). Sin embargo, no se han obtenido resultados similares en visión por ordenador, y la bibliografía no ofrece recomendaciones claras sobre qué arquitecturas de modelos funcionan bien. Por ejemplo, analizando la investigación reciente sobre el entrenamiento privado para CIFAR-10, Kurakin et al. (2022), Papernot et al. (2021) y Dörmann et al. (2021) utilizan variantes de modelos VGG poco profundos (Simonyan y Zisserman, 2015), mientras que Tramèr y Boneh (2021) utilizan ScatterNets (Oyallon y Mallat, 2015) para entrenar modelos lineales en características artesanales, logrando una impresionante precisión de prueba del 69,3 % con un presupuesto de privacidad ajustado de \\((3, 10^{-5})-DP\\). Por último, Klause et al. (2022) logran la precisión de prueba SOTA para 𝜀 ≤ 8 sin datos adicionales del 71,7% al entrenar una red residual superficial de 9 capas (He et al., 2016) bajo \\((7,5, 10^{-5})-DP\\).\nLa norma \\(l_2\\) del ruido añadido en la actualización DP-SGD escala proporcionalmente a la dimensión del gradiente (el número de parámetros). Esta observación ha llevado a muchos investigadores a creer que los modelos estándar sobreparametrizados funcionarán mal con DP-SGD, y en su lugar se centran en reducir la dimensión explícita o implícita de la actualización, ya sea mediante el uso de modelos pequeños/hand-crafted features (Tramèr y Boneh, 2021) o mediante técnicas de reducción de la dimensionalidad (Yu et al., 2021b,c). Otro obstáculo clave para el uso de modelos estándar para el entrenamiento privado en visión por computador ha sido que, con el fin de proporcionar garantías ajustadas de DP, DP-SGD requiere que los gradientes evaluados en diferentes ejemplos de entrenamiento sean independientes. Esto excluye el uso de cualquier método que permita la comunicación entre ejemplos de entrenamiento, como batch normalization (Ioffe y Szegedy, 2015), que hasta hace poco ha sido casi omnipresente en las arquitecturas de visión estándar (Brock et al., 2021b; Dosovitskiy et al., 2020; He et al., 2016; Tan y Le, 2019; Zagoruyko y Komodakis, 2016)."
  },
  {
    "objectID": "posts/08-Desglosando-prepare.py-del-proyecto-nanoGPT-Parte1/index.html",
    "href": "posts/08-Desglosando-prepare.py-del-proyecto-nanoGPT-Parte1/index.html",
    "title": "Desglosando el código de prepare.py del nanoGPT - Parte 1",
    "section": "",
    "text": "Hace unos días Andrej Karphaty publicó el proyecto nanoGPT, así que intenté reproducirlo en mi portátil. Sin embargo, durante el proceso de replicación me fallaron muchas cosas. Después de leer el código, me di cuenta de que tenía muchas lagunas conceptuales. Por lo tanto, decidí desglosar la primera parte del proyecto que consiste en descargar el dataset, tokenizarlo, hacer un memmap y generar los binarios train.bin y val.bin.\n\n💡 Importante:\n\n\nDescartar replicar este proyecto en Windows, porque Pytorch 2.0 no tiene soporte para este SO y si puedes también evita WSL.\nhttps://github.com/pytorch/pytorch/issues/90768\nInstalar Python 3.9 como mínimo y en caso no tengas una GPU te recomiendo LambdaLabs\n\nEmpecemos\nEn la versión original de prepare.py se importan las librerías tqdm, numpy, tiktoken y datasets. Sin embargo, yo en mi proceso de fortalecer los conceptos, importé estas dos funciones load_dataset_builder() y get_dataset_split_names() de manera adicional.\nfrom tqdm import tqdm\nimport numpy as np\nimport tiktoken\nfrom datasets import load_dataset, load_dataset_builder, get_dataset_split_names\n\nLibrerías:\n🗃️ tqdm : se usa para mostrar una barra de progreso durante iteraciones largas\n🗃️ numpy: servirá para trabajar con las matrices de forma eficiente\n🗃️ tiktoken: es el tokenizador opensource más rápido y lo liberó OpenAI\n🗃️ datasets: es la librería creada por Hugging Face que facilita el acceso a datasets populares de una manera sencilla\n\nMás adelante se aplicará la función ‘map()’ al dataset y se le proporcionará el parámetro ‘num_proc’ para definir el número de procesos que se ejecutarán en simultáneo.\n# un buen numero a usar es aproximadamente = CPU cores // 2\nnum_proc = 8\nAntes de descargar y empezar a manipular el dataset de openwebtext, es importante inspeccionarlo y obtener información sobre este dataset, como su descripción, características, size, etc. Para hacer esto, se usa la función ’load_dataset_builder()’\nds_builder = load_dataset_builder(\"openwebtext\")\n\nprint(\"Descripción de OWT: \\n\", ds_builder.info.description, \"\\n\")\nprint(\"Features de OWT: \\n\", ds_builder.info.features, \"\\n\")\nprint(\"Cita de OWT: \\n\", ds_builder.info.citation, \"\\n\")\nprint(\"Sitio Web de OWT: \\n\", ds_builder.info.homepage, \"\\n\")\n\nA partir de ahora nos referiremos a OpenWebText como OWT.\n\nPara listar los subconjuntos [’train’, ‘validation’, ‘test’] de OWT usamos la función ‘get_dataset_split_names()’. Este dataset por defecto sólo contiene ‘train’.\nprint(\"--Subconjuntos--\", get_dataset_split_names(\"openwebtext\"))\nLuego de haber inspeccionado y entendido más sobre el dataset de OWT, procedemos a descargarlo, este proceso se hace con la función ’load_dataset()’ y acá te recomiendo que tengas un como mínimo 200GB de espacio libre en disco, que conectes tu cable ethernet al portátil y te vayas por un café porque son 8M de documentos o 54GB que se irán almacenando en $HOME/.cache/huggingface/datasets/openwebtext/plain_text/1.0.0/…\ndataset = load_dataset(\"openwebtext\")\nprint(\"---Todos los subconjuntos y features---\", dataset)\nCon la función train_test_split() dividimos nuestro dataset en dos partes, uno llamado ‘train’ y otro ‘test’, el subconjunto ‘test’ equivale al 0.05% de OWT\nsplit_dataset = dataset[\"train\"].train_test_split(\ntest_size=0.0005,\nseed=2357,\nshuffle=True\n)\nEl contenido de la variable split_dataset sería este:\nDatasetDict({\n    train: Dataset({\n       features: ['text'],\n        num_rows: 8009762\n    })\n    test: Dataset({\n        features: ['text'],\n        num_rows: 4007\n    })\n})\nAhora, lo que se busca es tener un subconjunto de validación (’val’), para ello usamos la función pop() para transferir el contenido del subconjunto ‘test’ a ‘val’, y luego eliminar ‘test’ del conjunto original.\nsplit_dataset['val'] = split_dataset.pop('test') # renombramos test como val\nFinalmente el dataset de OWT se quedaría así:\n# mostramos en consola ambos dataset train y val\nprint(split_dataset)\n\nEl resultado de este print a split_dataset será: \nDatasetDict({\n    train: Dataset({\n        features: ['text'],\n        num_rows: 8009762\n    })\n    val: Dataset({\n        features: ['text'],\n        num_rows: 4007\n    })\n})\nEn el siguiente segmento se inicia el proceso de tokenizado del dataset, pero primero se instancia la variable ‘enc’ con el valor de codificación ‘gpt2’\nenc = tiktoken.get_encoding('gpt2')\n\ndef process(example):\n    # enconde_ordinary ignora cualquier token especial\n    ids = enc.encode_ordinary(example['text']) \n    # al final del texto agregamos el token '50256 o <|endoftext|>, para gpt2 bpe\n    ids.append(enc.eot_token)\n    # creamos un diccionario 'out' con los elementos id y len\n    out = {'ids': ids, 'len': len(ids)} \n    return out\n\n💡 Para ilustrar que hace la función process() les dejo este ejemplo:\n\n# Declarar una variable 'text' con una oración/sentence\ntext= \"Hola, me llamo Alexander y tú como te llamas?\"\nids= enc.encode_ordinary(text) # tokenizamos la variable\n# Estos serían los tokens que devuelve encode_ordinary\nprint(ids) # [39, 5708, 11, 502, 32660, 18811, 10009, 331, 256, 21356, 401, 78, 573, 32660, 17485, 30]\n\n# llamar a la función append() para agregar EOT token (50256 o <|endoftext|>)\nids.append(enc.eot_token)\nprint(ids) # [39, 5708, 11, 502, 32660, 18811, 10009, 331, 256, 21356, 401, 78, 573, 32660, 17485, 30, **50256**]\n\n# Llamar a la función decode() para descrifar ids\nprint(enc.decode(ids))  #Hola, me llamo Alexander y tú como te llamas?<|endoftext|>\n¿Recordarás que al principio mencionamos a la función map() ? Pues aquí la utilizamos con split_dataset y le pasamos los argumentos process(), remove_columns, desc y num_proc:\n# Aqui aplicamos la funcion process al dataset split_dataset creado lineas arriba\ntokenized = split_dataset.map(\n    process, # Funcion de tokenizado\n    remove_columns=['text'], # Luego de aplicar la función al dataset se elimina la columna text\n    desc=\"tokenizing the splits\", # Descripción que se mostrará en la barra de progreso\n    num_proc=num_proc # Número de procesos para generar un dataset local\n)\n\n# Este output sería un ejemplo ilustrativo:\n#tokenizing the splits #0: 100%|█████████████████████████████████████████████████████████████████████████████| 501/501 [00:01<00:00, 392.79ex/s]\n#tokenizing the splits #6: 100%|█████████████████████████████████████████████████████████████████████████████| 501/501 [00:01<00:00, 386.31ex/s]\n#tokenizing the splits #5: 100%|█████████████████████████████████████████████████████████████████████████████| 501/501 [00:01<00:00, 377.24ex/s]\n#tokenizing the splits #7: 100%|█████████████████████████████████████████████████████████████████████████████| 500/500 [00:01<00:00, 372.65ex/s]\n#tokenizing the splits #3: 100%|█████████████████████████████████████████████████████████████████████████████| 501/501 [00:01<00:00, 376.59ex/s]\n#tokenizing the splits #4: 100%|█████████████████████████████████████████████████████████████████████████████| 501/501 [00:01<00:00, 364.67ex/s]\n#tokenizing the splits #2: 100%|█████████████████████████████████████████████████████████████████████████████| 501/501 [00:01<00:00, 362.56ex/s]\n#tokenizing the splits #1: 100%|█████████████████████████████████████████████████████████████████████████████| 501/501 [00:01<00:00, 360.43ex/s]\nEn el segmento de abajo vemos que ya no existe la columna ‘text’:\nprint(tokenized)\n\ntrain: Dataset({\n        features: ['ids', 'len'],\n        num_rows: 8009762\n    })\n    val: Dataset({\n        features: ['ids', 'len'],\n        num_rows: 4007\n    })\n})\nPara terminar, juntamos todos los ids de cada dataset en un único archivo, que luego podremos usar para el training:\nfor split, dset in tokenized.items():\n    arr_len = np.sum(dset['len']) # calculamos el tamaño total de la matriz\n    filename = f'{split}.bin' # Acá usamos formatspec para crear de manera dinámica un archivo train.bin y val.bin\n    dtype = np.uint16 # Definimos este tipo np.uint16 para números enteros sin signo en el rango de 0 a 65535 (2^16 - 1) y como el valor máximo del token EOT es 50256 y es < 2^16 - 1 \n    arr = np.memmap(filename, dtype=dtype, mode='w+', shape=(arr_len,)) # En la variable arr es del tipo memoria mapeada, esto es util porque se trabaja con archivos grandes \n\n    print(f\"writing {filename}...\") # Esto indica el nombre del archivo en el que se está escribiendo la matriz por ejemplo train.bin o val.bin\n    idx = 0 # Establecemos el valor 0 para iniciar\n    for example in tqdm(dset): # Con tqdm tendremos una barra de progreso según vayamos iterando sobre cada elemento de dset\n        arr[idx : idx + example['len']] = example['ids'] # Esto es slicing para asignar los valores de example['ids'] a una sección de la matriz 'arr'\n        idx += example['len'] # sumamos el valor actual de example['len']\n    arr.flush() # por último usamos la función flush() para vaciar el buffer, es decir escribiremos físicamente todos los datos en el disco duro.\nAl final tendremos :\n\ntrain.bin de ~17GB y val.bin ~8.5MB\ntrain tiene ~9B tokens (9,035,582,198)\nval tiene ~4M tokens (4,434,897)\nLuego leeremos los archivos .bin con numpy de la siguiente manera:\n\nm = np.memmap('train.bin', dtype=np.uint16, mode='r')"
  }
]