---
title: "Desglosando el cÃ³digo de prepare.py del nanoGPT - Parte 1"
author: "Alexander Zelada"
date: "2023-01-07"
categories: [NLP, GPT, Pytorch]
image: "Bienvenida.png"
---
>GPT-2, fue entrenado simplemente para predecir la siguiente palabra en 40GB de texto de internet

Hace unos dÃ­as **[Andrej Karphaty](https://karpathy.ai/)** publicÃ³ el proyecto [**nanoGPT**](https://github.com/karpathy/nanoGPT), asÃ­ que intentÃ© reproducirlo en mi portÃ¡til. Sin embargo, durante el proceso de replicaciÃ³n me fallaron muchas cosas. DespuÃ©s de leer el cÃ³digo, me di cuenta de que tenÃ­a muchas lagunas conceptuales. Por lo tanto, decidÃ­ desglosar la primera parte del proyecto que consiste en descargar el dataset, tokenizarlo, hacer un memmap y generar los binarios train.bin y val.bin.

> ðŸ’¡ Importante:


> Descartar replicar este proyecto en Windows, porque ***Pytorch 2.0*** no tiene soporte para este SO y si puedes tambiÃ©n evita WSL.
> 
> 
> [https://github.com/pytorch/pytorch/issues/90768](https://github.com/pytorch/pytorch/issues/90768)
> 
> Instalar Python 3.9 como mÃ­nimo y en caso no tengas una GPU te recomiendo [**LambdaLabs**](https://lambdalabs.com/)
> 

**Empecemos**

En la versiÃ³n original de [prepare.py](http://prepare.py) se importan las librerÃ­as *tqdm, numpy, tiktoken* y **datasets*. S*in embargo, yo en mi proceso de fortalecer los conceptos, importÃ© estas dos funciones  *load_dataset_builder* y *get_dataset_split_names* de manera adicional*.*

```python
from tqdm import tqdm
import numpy as np
import tiktoken
from datasets import load_dataset, load_dataset_builder, get_dataset_split_names
```

> LibrerÃ­as:
> 
>
> ðŸ—ƒï¸ **tqdm** :  se usa para mostrar una barra de progreso durante iteraciones largas
> 
> ðŸ—ƒï¸ **numpy**: servirÃ¡ para trabajar con las matrices de forma eficiente
> 
> ðŸ—ƒï¸ **tiktoken**: es el tokenizador opensource mÃ¡s rÃ¡pido y lo liberÃ³ OpenAI
> 
> ðŸ—ƒï¸ **datasets**: es la librerÃ­a creada por Hugging Face que facilita el acceso a datasets populares de una manera sencilla
> 

MÃ¡s adelante se aplicarÃ¡ la funciÃ³n ***â€˜map()â€™*** al dataset y se le proporcionarÃ¡ el parÃ¡metro â€˜**num_proc**â€™ para definir el nÃºmero de procesos que se ejecutarÃ¡n en simultÃ¡neo.

```python
# un buen numero a usar es aproximadamente = CPU cores // 2
num_proc = 8
```

Antes de descargar y empezar a manipular el dataset de **[openwebtext](https://huggingface.co/datasets/openwebtext)**, es importante inspeccionarlo y obtener informaciÃ³n sobre este dataset, como su descripciÃ³n, caracterÃ­sticas, size, etc. Para hacer esto, se usa la funciÃ³n â€˜***load_dataset_builder()â€™***

```python
ds_builder = load_dataset_builder("openwebtext")

print("DescripciÃ³n de OWT: \n", ds_builder.info.description, "\n")
print("Features de OWT: \n", ds_builder.info.features, "\n")
print("Cita de OWT: \n", ds_builder.info.citation, "\n")
print("Sitio Web de OWT: \n", ds_builder.info.homepage, "\n")
```

> A partir de ahora nos referiremos a OpenWebText como OWT.
> 

 

Para listar los subconjuntos [â€™trainâ€™, â€˜validationâ€™, â€˜testâ€™] de OWT usamos la funciÃ³n â€˜**get_dataset_split_names()**â€™. *Este dataset por defecto sÃ³lo contiene â€˜trainâ€™*.

```python
print("--Subconjuntos--", get_dataset_split_names("openwebtext"))
```

Luego de haber inspeccionado y entendido mÃ¡s sobre el dataset de OWT, procedemos a descargarlo, este proceso se hace con la funciÃ³n ****â€˜**load_dataset()â€™** y acÃ¡ te recomiendo que tengas un como mÃ­nimo 200GB de espacio libre en disco, que conectes tu cable ethernet al portÃ¡til y te vayas por un cafÃ© porque son 8M de documentos o 54GB que se irÃ¡n almacenando en $HOME/.cache/huggingface/datasets/openwebtext/plain_text/1.0.0/â€¦

```python
dataset = load_dataset("openwebtext")
print("---Todos los subconjuntos y features---", dataset)
```

Con la funciÃ³n ***train_test_split***() dividimos nuestro dataset en dos partes, uno llamado â€˜trainâ€™ y otro â€˜testâ€™, el subconjunto â€˜testâ€™ equivale al 0.05% de OWT

```python
split_dataset = dataset["train"].train_test_split(
test_size=0.0005,
seed=2357,
shuffle=True
)
```

El contenido de la variable split_dataset serÃ­a este:

```python
DatasetDict({
    train: Dataset({
       features: ['text'],
        num_rows: 8009762
    })
    test: Dataset({
        features: ['text'],
        num_rows: 4007
    })
})
```

Ahora, lo que se busca es tener un subconjunto de validaciÃ³n (â€™valâ€™), para ello usamos la funciÃ³n ***pop()*** para transferir el contenido del subconjunto â€˜testâ€™ a â€˜valâ€™, y luego eliminar â€˜testâ€™ del conjunto original.

```python
split_dataset['val'] = split_dataset.pop('test') # renombramos test como val
```

Finalmente el dataset de OWT se quedarÃ­a asÃ­:

```python
# mostramos en consola ambos dataset train y val
print(split_dataset)

El resultado de este print a split_dataset serÃ¡: 
DatasetDict({
    train: Dataset({
        features: ['text'],
        num_rows: 8009762
    })
    val: Dataset({
        features: ['text'],
        num_rows: 4007
    })
})
```

En el siguiente segmento se inicia el proceso de ***tokenizado*** del dataset, pero primero se instancia la variable â€˜***enc***â€™ con el valor de codificaciÃ³n â€˜***gpt2***â€™

```python
enc = tiktoken.get_encoding('gpt2')

def process(example):
    # enconde_ordinary ignora cualquier token especial
    ids = enc.encode_ordinary(example['text']) 
    # al final del texto agregamos el token '50256 o <|endoftext|>, para gpt2 bpe
    ids.append(enc.eot_token)
    # creamos un diccionario 'out' con los elementos id y len
    out = {'ids': ids, 'len': len(ids)} 
    return out
```

> ðŸ’¡ Para ilustrar que hace la funciÃ³n ***process()** les dejo este ejemplo:*

```python
# Declarar una variable 'text' con una oraciÃ³n/sentence
text= "Hola, me llamo Alexander y tÃº como te llamas?"
ids= enc.encode_ordinary(text) # tokenizamos la variable
# Estos serÃ­an los tokens que devuelve encode_ordinary
print(ids) # [39, 5708, 11, 502, 32660, 18811, 10009, 331, 256, 21356, 401, 78, 573, 32660, 17485, 30]

# llamar a la funciÃ³n append() para agregar EOT token (50256 o <|endoftext|>)
ids.append(enc.eot_token)
print(ids) # [39, 5708, 11, 502, 32660, 18811, 10009, 331, 256, 21356, 401, 78, 573, 32660, 17485, 30, **50256**]

# Llamar a la funciÃ³n decode() para descrifar ids
print(enc.decode(ids))  #Hola, me llamo Alexander y tÃº como te llamas?<|endoftext|>

```

Â¿RecordarÃ¡s que al principio mencionamos a la funciÃ³n map() ? Pues aquÃ­ la utilizamos con split_dataset y le pasamos los argumentos process(), remove_columns, desc y num_proc:

```python
# Aqui aplicamos la funcion process al dataset split_dataset creado lineas arriba
tokenized = split_dataset.map(
    process, # Funcion de tokenizado
    remove_columns=['text'], # Luego de aplicar la funciÃ³n al dataset se elimina la columna text
    desc="tokenizing the splits", # DescripciÃ³n que se mostrarÃ¡ en la barra de progreso
    num_proc=num_proc # NÃºmero de procesos para generar un dataset local
)

# Este output serÃ­a un ejemplo ilustrativo:
#tokenizing the splits #0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 501/501 [00:01<00:00, 392.79ex/s]
#tokenizing the splits #6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 501/501 [00:01<00:00, 386.31ex/s]
#tokenizing the splits #5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 501/501 [00:01<00:00, 377.24ex/s]
#tokenizing the splits #7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:01<00:00, 372.65ex/s]
#tokenizing the splits #3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 501/501 [00:01<00:00, 376.59ex/s]
#tokenizing the splits #4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 501/501 [00:01<00:00, 364.67ex/s]
#tokenizing the splits #2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 501/501 [00:01<00:00, 362.56ex/s]
#tokenizing the splits #1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 501/501 [00:01<00:00, 360.43ex/s]
```

En el segmento de abajo vemos que ya no existe la columna â€˜textâ€™:

```python
print(tokenized)

train: Dataset({
        features: ['ids', 'len'],
        num_rows: 8009762
    })
    val: Dataset({
        features: ['ids', 'len'],
        num_rows: 4007
    })
})
```

Para terminar,  juntamos todos los ids de cada dataset en un Ãºnico archivo, que luego podremos usar para el training:

```python
for split, dset in tokenized.items():
    arr_len = np.sum(dset['len']) # calculamos el tamaÃ±o total de la matriz
    filename = f'{split}.bin' # AcÃ¡ usamos formatspec para crear de manera dinÃ¡mica un archivo train.bin y val.bin
    dtype = np.uint16 # Definimos este tipo np.uint16 para nÃºmeros enteros sin signo en el rango de 0 a 65535 (2^16 - 1) y como el valor mÃ¡ximo del token EOT es 50256 y es < 2^16 - 1 
    arr = np.memmap(filename, dtype=dtype, mode='w+', shape=(arr_len,)) # En la variable arr es del tipo memoria mapeada, esto es util porque se trabaja con archivos grandes 

    print(f"writing {filename}...") # Esto indica el nombre del archivo en el que se estÃ¡ escribiendo la matriz por ejemplo train.bin o val.bin
    idx = 0 # Establecemos el valor 0 para iniciar
    for example in tqdm(dset): # Con tqdm tendremos una barra de progreso segÃºn vayamos iterando sobre cada elemento de dset
        arr[idx : idx + example['len']] = example['ids'] # Esto es slicing para asignar los valores de example['ids'] a una secciÃ³n de la matriz 'arr'
        idx += example['len'] # sumamos el valor actual de example['len']
    arr.flush() # por Ãºltimo usamos la funciÃ³n flush() para vaciar el buffer, es decir escribiremos fÃ­sicamente todos los datos en el disco duro.
```

Al final tendremos :

> train.bin de ~17GB y val.bin ~8.5MB
>
> train tiene ~9B tokens (9,035,582,198)
>
> val tiene ~4M tokens (4,434,897)
>
>Luego leeremos los archivos .bin con numpy de la siguiente manera:
```python
m = np.memmap('train.bin', dtype=np.uint16, mode='r')
```