---
title: "Unlocking High-Accuracy Differentially Private Image Classification through Scale - Parte 2"
author: "Alexander Zelada"
date: "2022-12-09"
categories: [Privacy, Paper, Privacy-preserving, DeepMind]
image: "deepmind.jpeg"
---
>(DP-SGD), el m√©todo de entrenamiento de DP m√°s popular para el aprendizaje profundo, realiza esta protecci√≥n mediante la inyecci√≥n de ruido durante el entrenamiento.

Voy a trabajar en la traducci√≥n del paper ["Unlocking High-Accuracy Differentially Private Image Classification through Scale"](https://www.deepmind.com/blog/unlocking-high-accuracy-differentially-private-image-classification-through-scale), este documento consta de 6 secciones y cada post ser√° una de ellas. Como parte de mi aprendizaje considero importante generar informaci√≥n en mi lengua materna.


## **2.- Background**

**2.1. Privacidad Diferencial DP**

La privacidad diferencial es una garant√≠a de privacidad formal que se aplica a los algoritmos de an√°lisis de datos aleatorios. Por construcci√≥n, los algoritmos diferencialmente privados evitan que un adversario que observa el resultado de un c√≥mputo deduzca cualquier propiedad relacionada con data points individuales en los datos de entrada utilizados durante el c√≥mputo.

La fuerza de esta garant√≠a est√° controlada por dos par√°metros:  $\epsilon>0$ y  $\delta \in [0,1]$. En t√©rminos generales, $\epsilon$ limita la relaci√≥n logar√≠tmica de verosimilitud de cualquier resultado particular que se puede obtener al ejecutar el algoritmo en dos conjuntos de datos que difieren en un solo punto de datos, y $\delta$ es una peque√±a probabilidad que limita la aparici√≥n de resultados poco frecuentes que violan este l√≠mite. La garant√≠a de privacidad se fortalece a medida que ambos par√°metros se reducen. Una regla general est√°ndar establece que, para obtener una privacidad significativa, $\epsilon$ debe ser una constante peque√±a mientras que $\delta$ debe ser menor que $1/N$, donde $N$ es el tama√±o del conjunto de datos de entrada. M√°s formalmente, tenemos lo siguiente.

**Definici√≥n 2.1** (Differential Privacy ([Dwork et al., 2006](https://people.csail.mit.edu/asmith/PS/sensitivity-tcc-final.pdf))). Sea $A:D \longmapsto S$ un algoritmo aleatorio, y sea :  $\epsilon>0$ y  $\delta \in [0,1]$. Decimos que $A$ es $(\epsilon, \delta)-DP$ si para dos conjuntos de datos vecinos cualesquiera $D, D{'} \in D$
 difieren en un solo elemento, tenemos que:

$$
\begin{equation} \lor S \subset S, P[A(D)\in S] \leq exp(\epsilon)P[A(D{'})\in S] + \delta  \end{equation}
$$

La protecci√≥n de la privacidad que brinda DP se mantiene bajo un modelo de amenaza extremadamente fuerte: las inferencias sobre las personas est√°n protegidas incluso frente a un adversario que tiene pleno conocimiento del algoritmo DP, poder computacional ilimitado y *arbitrary side knowledge* sobre los datos de entrada. Adem√°s, DP satisface una serie de propiedades atractivas desde el punto de vista del dise√±o de algoritmos, incluida la conservaci√≥n bajo procesamiento posterior y una degradaci√≥n suave con m√∫ltiple accesos a los mismos datos. Estas propiedades se explotan en la construcci√≥n de algoritmos DP complejos basados en la combinaci√≥n de peque√±os bloques de construcci√≥n que inyectan ruido cuidadosamente calibrado en las operaciones que acceden a los datos. La magnitud del ruido requerido para satisfacer la garant√≠a de privacidad aumenta con la fuerza de los par√°metros de privacidad, lo que lleva a una compensaci√≥n inevitable entre utilidad y privacidad, como lo ilustra la Ley Fundamental de Recuperaci√≥n de Informaci√≥n [(Dwork and Roth, 2014)](https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf).

Juntos, la solidez de la garant√≠a formal que brinda y la variedad de herramientas disponibles para la construcci√≥n de algoritmos de DP han llevado a la creciente adopci√≥n de DP como un est√°ndar de oro para el aprendizaje autom√°tico que preserva la privacidad. Para problemas de *convex learning*, existe una variedad de m√©todos para obtener algoritmos diferencialmente privados, incluida la perturbaci√≥n de salida [(Chaudhuri et al., 2011](https://www.jmlr.org/papers/volume12/chaudhuri11a/chaudhuri11a.pdf); [Wu et al., 2017](https://arxiv.org/pdf/1606.04722.pdf)), la perturbaci√≥n objetiva ([Chaudhuri et al., 2011](https://www.jmlr.org/papers/volume12/chaudhuri11a/chaudhuri11a.pdf); [Kifer et al., 2012](http://proceedings.mlr.press/v23/kifer12/kifer12.pdf)) y la perturbaci√≥n de gradiente ([Bassily et al., 2014](https://arxiv.org/pdf/1405.7085.pdf); [Song et al., 2013](https://cseweb.ucsd.edu/~kamalika/pubs/scs13.pdf)) .

La naturaleza de los problemas convexos permite el an√°lisis formal de la utilidad de privacidad que ofrecen estos algoritmos, y en la actualidad existen grandes clases de problemas para los cuales se conocen algoritmos que logran (casi) equilibrios √≥ptimos de utilidad de privacidad ([Asi et al., 2021](https://arxiv.org/pdf/2103.01516.pdf); [Bassily et al., 2014](https://arxiv.org/pdf/1405.7085.pdf); [Feldman et al., 2020](https://arxiv.org/pdf/2005.04763.pdf); [Song et al., 2021](https://proceedings.mlr.press/v130/song21a.html); [Talwar et al., 2015](https://papers.nips.cc/paper/2015/file/52d080a3e172c33fd6886a37e7288491-Paper.pdf)). Para los problemas de aprendizaje no convexos, la gama de algoritmos disponibles es m√°s limitada y las ventajas y desventajas de la privacidad y la utilidad son m√°s dif√≠ciles de analizar te√≥ricamente. No obstante, para tales problemas existen dos familias de algoritmos que han demostrado lograr compensaciones razonables de privacidad-utilidad-c√≥mputo en la pr√°ctica; perturbaci√≥n de gradiente aplicada a optimizadores est√°ndar como SGD ([Abadi et al., 2016](https://arxiv.org/pdf/1607.00133.pdf)),  y agregaci√≥n privada de *teacher ensembles ([Papernot et al., 2018](https://arxiv.org/pdf/1802.08908.pdf)).* En este trabajo nos centramos en el primero, que es el m√°s utilizado.

**2.2.** **Descenso de gradiente estoc√°stico diferencialmente privado (DP-SGD)**

En este trabajo, suponemos que el algoritmo diferencialmente privado A, es un algoritmo de aprendizaje que mapea un conjunto de datos de entrenamiento: $D = \lbrace(x_i, y_i)\rbrace_{1\leq i\leq N}$ a un vector de par√°metros de la red neuronal aprendida $w \in S = R^{p}$. Sea $\mathcal{L}(w,x,y)$ el objetivo de aprendizaje (por ejemplo, la p√©rdida de entrop√≠a cruzada), dados los par√°metros del modelo $w$, la entrada ejemplo $x$ y la etiqueta $y$. Por comodidad, utilizamos la notaci√≥n abreviada $l_i(w) = \mathcal{L}(w,x_i,y_i)$.

En la configuraci√≥n non-private, una actualizaci√≥n de par√°metros utilizando el Descenso de Gradiente Estoc√°stico (SGD) en la iteraci√≥n $t$ extrae $B$ ejemplos al azar del conjuntos de datos, y realiza una actualizaci√≥n de la forma:

$$
w^{(t+1)}= w^{(t)} - \eta_{t}\frac{1}{B} \sum_{i \in \beta_t}\nabla l_i(w^{(t)}),
$$

Donde $\eta_t$ es el step-size para una actualizaci√≥n $t^{th}$, $\nabla$ denota el operador de gradiente, y $B_t$ representa el conjunto de ejemplos muestreados en la iteraci√≥n $t$ con $|B_t| = B$. Para que este algoritmo sea diferencialmente privado, aplicamos las siguientes modificaciones. En primer lugar, el gradiente de cada ejemplo del mini-batch es clipped a una norma m√°xima $C$, y en segundo lugar, se a√±ade ruido gaussiano con desviaci√≥n est√°ndar proporcional a $C$ se a√±ade a la media de los grandientes clipped. 

Sea 

$$
clip_x:v\in R^{p}\longmapsto min\lbrace{1,\frac{c}{||v||_2}\rbrace}.v\in R^{p} 
$$

denote la funci√≥n de clipping que reescala su entrada para que la salida tenga una norma m√°xima $l_2$ de $C$. El nuevo step de actualizaci√≥n es:

$$
\begin{equation}  w^{(t+1)}= w^{(t)} -\eta_{t} \{\frac{1}{B}\sum_{i \in \beta_t}clip_c\left(\nabla l_i(w^{(t)})\right) +  \frac{\sigma C}{B}\xi\}, \end{equation}
$$

Donde $\xi \sim N(0,I_p)$ es una variable aleatoria gaussiana est√°ndar de $p$ dimensiones y $\sigma$ especifica la desviaci√≥n est√°ndar del ruido a√±adido. El algoritmo resultante se llama ***Differentially Private-Stochastic Gradient Descent (DP-SGD)*** ([Abadi et al., 2016](https://arxiv.org/pdf/1607.00133.pdf)). Intuitivamente, realizar una actualizaci√≥n del modelo utilizando la ecuaci√≥n proporciona privacidad diferencial porque la adici√≥n de ruido gaussiano con desviaci√≥n est√°ndar proporcional a $C$ es suficiente para enmascarar la contribuci√≥n de cualquier ejemplo individual cuyo gradiente recortado tenga una norma menor o igual a $C$. Aunque utilizamos el SGD privatizado como nuestro optimizador a lo largo de este trabajo, tambi√©n se puede utilizar un m√©todo de privatizaci√≥n similar en combinaci√≥n con otros algoritmos de optimizaci√≥n de primer orden, como SGD con momentum o Adam ([McMahan et al., 2018a](https://arxiv.org/pdf/1812.06210.pdf)).

A lo largo de este trabajo utilizamos una versi√≥n modificada de DP-SGD en la que el gradiente privatizado est√° normalizado por $C$:

$$
\begin{equation} w^{(t+1)}= w^{(t)} -\eta_{t} \{\frac{1}{B}\sum_{i \in \beta_t}\frac{1}{C} clip_c\left(\nabla l_i(w^{(t)})\right) + \frac{\sigma}{B}\xi\}, \end{equation}
$$

Esta es una reparametrizaci√≥n de la ecuaci√≥n (2) en la que la tasa de aprendizaje $\eta_t$ absorbe un factor de $C$. Esto no tiene ning√∫n efecto sobre las garant√≠as de privacidad, pero asegura que la norma de clipping no influya en la escala de la actualizaci√≥n, lo que simplifica el ajuste de los hiperpar√°metros. Tenga en cuenta que para preservar las garant√≠as de DP, debemos dividir por $C$ despu√©s de la operaci√≥n de clipping. El Ap√©ndice A.1 proporciona m√°s detalles sobre nuestra implementaci√≥n de DP-SGD, incluida una descripci√≥n de nuestro enfoque de procesamiento por virtual batching para permitir el entrenamiento con grandes batch sizes.

**Privacy accounting.** La garant√≠a de privacidad de DP-SGD est√° determinada por tres par√°metros: la desviaci√≥n est√°ndar $\sigma$, el ratio de muestreo $q=B/N$ y el n√∫mero de iteraciones de entrenamiento $T$. En la pr√°ctica, el presupuesto de privacidad $(\epsilon, \delta)$ suele ser fijo, y estos tres hiperpar√°metros se eligen para proporcionar el mejor rendimiento posible dentro de este presupuesto. Tambi√©n puede haber restricciones pr√°cticas adicionales (por ejemplo, el presupuesto de c√°lculo m√°ximo disponible). El proceso de calibraci√≥n de la privacidad se realiza mediante un contador de privacidad: un algoritmo num√©rico que proporciona l√≠mites superiores ajustados para el presupuesto de privacidad en funci√≥n de los hiperpar√°metros ([Abadi et al., 2016](https://arxiv.org/pdf/1607.00133.pdf)), que a su vez puede combinarse con rutinas de optimizaci√≥n num√©rica para optimizar un hip√©rparametro dado el presupuesto de privacidad y los otros dos hiperpar√°metros.

En este trabajo utilizamos el m√©todo de contabilidad para DP-SGD propuesto por M[ironov et al. (2019)](https://arxiv.org/pdf/1908.10530.pdf) e implementado en TensorFlow Privacy ([Google, 2018](https://github.com/tensorflow/privacy)). Esta privacidad contable se basa en un an√°lisis de ‚Äúcomposici√≥n‚Äù a trav√©s de iteraciones, que nos permite liberar no solo el modelo final, sino tambi√©n cada modelo intermedio obtenido durante el entrenamiento (bajo el mismo presupuesto de privacidad)

**2.3. Retos de DP-SGD**

Como describimos anteriormente, existen tres diferencias clave entre DP-SGD y SGD no privado: (1) Los gradientes por ejemplo se recortan (clipped) a una norma m√°xima de $l_2$ antes de promediarlos, (2) se a√±ade ruido gaussiano a la media de los gradientes clipped, y (3) el n√∫mero m√°ximo de actualizaciones permitidas dentro del presupuesto de privacidad est√° limitado y depende del batch size/ruido a√±adido. Estas diferencias plantean una serie de retos:

**Ajuste y regularizaci√≥n de hiperpar√°metros.** El ruido agregado a la estimaci√≥n del gradiente en la actualizaci√≥n de DP-SGD (Ecuaci√≥n (3)) es una barrera significativa para la optimizaci√≥n eficiente, y si reducimos la escala de este ruido, el n√∫mero de iteraciones de entrenamiento permitidas dentro del presupuesto de privacidad disminuye. Esta restricci√≥n altera los valores √≥ptimos de hiperpar√°metros clave como el batch size/learning rate, y los valores por defecto del entrenamiento no privado pueden ser muy sub√≥ptimos ([Papernot et al., 2021](https://arxiv.org/pdf/2007.14191.pdf)). En consecuencia, DP-SGD requiere un ajuste cuidadoso de los hiperpar√°metros.

En nuestros experimentos tambi√©n descubrimos que, al entrenar con DP-SGD, las mejoras en el training accuracy suelen traducirse directamente en una mejora de la generalizaci√≥n, sin necesidad de una fuerte regularizaci√≥n. Inspirados en esta observaci√≥n, nuestra filosof√≠a es que los m√©todos que reducen el n√∫mero de iteraciones de entrenamiento necesarias para alcanzar un high training accuracy en el entrenamiento no privado probablemente mejoren el test accuracy alcanzado en el entrenamiento privado. De acuerdo con este enfoque, suele ser beneficioso eliminar los m√©todos de regularizaci√≥n expl√≠citos.

**Bias y Varianza de la actualizaci√≥n DP-SGD.**  El estimador de gradiente utilizado por DP-SGD est√° biased debido al uso de gradiente clipping por ejemplo y, en general, no corresponde al gradiente de ninguna funci√≥n diferenciable ([Song et al., 2021](https://proceedings.mlr.press/v130/song21a.html)). Y lo que es m√°s importante, la norma de clipping $C$ introduce una compensaci√≥n entre sesgo y varianza ([Chen et al., 2020](https://arxiv.org/pdf/2006.15429.pdf); [Thakkar et al., 2021](https://arxiv.org/pdf/1905.03871.pdf)).  Esto puede verse en la actualizaci√≥n DP-SGD que se muestra en la ecuaci√≥n (3). Cuando $C$ es muy grande, $clip_c$ es la funci√≥n identidad, por lo que el gradiente privatizado es un estimador unbiased del verdadero gradiente, pero el gradiente clipped $\frac {1}{c} clip_c(\nabla l_i(w^{(t)}))$ es muy peque√±o en comparaci√≥n con el ruido (que es independiente de $C$) - en general, la estimaci√≥n del gradiente privatizado tiene un bias bajo y una varianza alta. Por el contrario, si $C$ es peque√±o, la operaci√≥n de clipping introduce un bias, pero el gradiente clipped $\frac {1}{c} clip_c(\nabla l_i(w^{(t)}))$ es mayor, y por lo tanto no es necesariamente peque√±o en comparacion con el ruido- en general, la estimaci√≥n del gradiente privatizado tiene un bias alto y una varianza baja. Tenga en cuenta que cuando $C$ es muy peque√±o (m√°s peque√±o que la norma de gradiente m√°s peque√±a por ejemplo), reducir a√∫n m√°s $C$ no cambia $\frac {1}{c} clip_c(\nabla l_i(w^{(t)}))$, lo que indica que el bias y la varianza en la actualizaci√≥n se aproximan a una constante a medida que $C \longmapsto 0$.  Curiosamente, trabajos anteriores han observado que amplios rangos de la norma de clipping $C$ pueden proporcionar un rendimiento casi √≥ptimo siempre que (i) la norma de clipping sea lo suficientemente peque√±a, y (ii) el learning-rate se reescalen en consecuecia [(Kurakin et al., 2022](https://arxiv.org/pdf/2201.12328.pdf); [Li et al., 2021](https://arxiv.org/pdf/2110.05679.pdf)). Esto sugiere que reducir la varianza introducida por el ruido puede ser m√°s importante que reducir el bias introducido por el clipping.

**Hacer que los modelos est√°ndar funcionen.** El entrenamiento diferencialmente privado ha obtenido recientemente resultados prometedores con arquitecturas est√°ndar en NLP, tanto al entrenar un modelo BERT ([Devlin et al., 2018](https://arxiv.org/pdf/1810.04805.pdf)) a partir de una inicializaci√≥n aleatoria ([Anil et al., 2018](https://arxiv.org/pdf/2108.01624.pdf)) como al afinar un gran modelo de lenguaje Transformer ([Vaswani et al., 2017](https://arxiv.org/pdf/1706.03762.pdf)) a partir de un conjunto de par√°metros preentrenados ([Li et al., 2021](https://arxiv.org/pdf/2110.05679.pdf); [Yu et al., 2021a](https://arxiv.org/pdf/2110.06500.pdf)). Sin embargo, no se han obtenido resultados similares en visi√≥n por ordenador, y la bibliograf√≠a no ofrece recomendaciones claras sobre qu√© arquitecturas de modelos funcionan bien. Por ejemplo, analizando la investigaci√≥n reciente sobre el entrenamiento privado para CIFAR-10, [Kurakin et al. (2022)](https://arxiv.org/pdf/2201.12328.pdf), [Papernot et al. (2021)](https://arxiv.org/pdf/2007.14191.pdf) y [D√∂rmann et al. (2021)](https://arxiv.org/pdf/2110.06255.pdf) utilizan variantes de modelos VGG poco profundos ([Simonyan y Zisserman, 2015](https://arxiv.org/pdf/1409.1556.pdf)), mientras que [Tram√®r y Boneh (2021)](https://arxiv.org/pdf/2011.11660.pdf) utilizan ScatterNets ([Oyallon y Mallat, 2015](https://arxiv.org/pdf/1412.8659.pdf)) para entrenar modelos lineales en caracter√≠sticas artesanales, logrando una impresionante precisi√≥n de prueba del 69,3 % con un presupuesto de privacidad ajustado de $(3, 10^{-5})-DP$. Por √∫ltimo, [Klause et al. (2022)](https://arxiv.org/pdf/2203.00324.pdf) logran la precisi√≥n de prueba SOTA para ùúÄ ‚â§ 8 sin datos adicionales del 71,7% al entrenar una red residual superficial de 9 capas [(He et al., 2016](https://arxiv.org/pdf/1512.03385.pdf)) bajo $(7,5, 10^{-5})-DP$.

La norma $l_2$ del ruido a√±adido en la actualizaci√≥n DP-SGD escala proporcionalmente a la dimensi√≥n del gradiente (el n√∫mero de par√°metros). Esta observaci√≥n ha llevado a muchos investigadores a creer que los modelos est√°ndar sobreparametrizados funcionar√°n mal con DP-SGD, y en su lugar se centran en reducir la dimensi√≥n expl√≠cita o impl√≠cita de la actualizaci√≥n, ya sea mediante el uso de modelos peque√±os/hand-crafted features ([Tram√®r y Boneh, 2021](https://arxiv.org/pdf/2011.11660.pdf)) o mediante t√©cnicas de reducci√≥n de la dimensionalidad ([Yu et al., 2021b,c](https://arxiv.org/pdf/2102.12677.pdf)). Otro obst√°culo clave para el uso de modelos est√°ndar para el entrenamiento privado en visi√≥n por computador ha sido que, con el fin de proporcionar garant√≠as ajustadas de DP, DP-SGD requiere que los gradientes evaluados en diferentes ejemplos de entrenamiento sean independientes. Esto excluye el uso de cualquier m√©todo que permita la comunicaci√≥n entre ejemplos de entrenamiento, como batch normalization ([Ioffe y Szegedy, 2015](https://arxiv.org/pdf/1502.03167.pdf)), que hasta hace poco ha sido casi omnipresente en las arquitecturas de visi√≥n est√°ndar ([Brock et al., 2021b](https://arxiv.org/pdf/2102.06171.pdf); [Dosovitskiy et al., 2020](https://arxiv.org/pdf/2010.11929.pdf); [He et al., 2016](https://arxiv.org/pdf/1512.03385.pdf); [Tan y Le, 2019](https://arxiv.org/pdf/1905.11946.pdf); [Zagoruyko y Komodakis, 2016](https://arxiv.org/pdf/1605.07146.pdf)).