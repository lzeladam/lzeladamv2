---
title: "Unlocking High-Accuracy Differentially Private Image Classification through Scale - Parte 2"
author: "Alexander Zelada"
date: "2022-12-09"
categories: [Privacy, Paper, Privacy-preserving, DeepMind]
image: "deepmind.jpeg"
---
>(DP-SGD), el método de entrenamiento de DP más popular para el aprendizaje profundo, realiza esta protección mediante la inyección de ruido durante el entrenamiento.

Voy a trabajar en la traducción del paper ["Unlocking High-Accuracy Differentially Private Image Classification through Scale"](https://www.deepmind.com/blog/unlocking-high-accuracy-differentially-private-image-classification-through-scale), este documento consta de 6 secciones y cada post será una de ellas. Como parte de mi aprendizaje considero importante generar información en mi lengua materna.


## **2.- Background**

**2.1. Privacidad Diferencial DP**

La privacidad diferencial es una garantía de privacidad formal que se aplica a los algoritmos de análisis de datos aleatorios. Por construcción, los algoritmos diferencialmente privados evitan que un adversario que observa el resultado de un cómputo deduzca cualquier propiedad relacionada con data points individuales en los datos de entrada utilizados durante el cómputo.

La fuerza de esta garantía está controlada por dos parámetros:  $\epsilon>0$ y  $\delta \in [0,1]$. En términos generales, $\epsilon$ limita la relación logarítmica de verosimilitud de cualquier resultado particular que se puede obtener al ejecutar el algoritmo en dos conjuntos de datos que difieren en un solo punto de datos, y $\delta$ es una pequeña probabilidad que limita la aparición de resultados poco frecuentes que violan este límite. La garantía de privacidad se fortalece a medida que ambos parámetros se reducen. Una regla general estándar establece que, para obtener una privacidad significativa, $\epsilon$ debe ser una constante pequeña mientras que $\delta$ debe ser menor que $1/N$, donde $N$ es el tamaño del conjunto de datos de entrada. Más formalmente, tenemos lo siguiente.

**Definición 2.1** (Differential Privacy ([Dwork et al., 2006](https://people.csail.mit.edu/asmith/PS/sensitivity-tcc-final.pdf))). Sea $A:D \longmapsto S$ un algoritmo aleatorio, y sea :  $\epsilon>0$ y  $\delta \in [0,1]$. Decimos que $A$ es $(\epsilon, \delta)-DP$ si para dos conjuntos de datos vecinos cualesquiera $D, D{'} \in D$
 difieren en un solo elemento, tenemos que:

$$
\begin{equation} \lor S \subset S, P[A(D)\in S] \leq exp(\epsilon)P[A(D{'})\in S] + \delta  \end{equation}
$$

La protección de la privacidad que brinda DP se mantiene bajo un modelo de amenaza extremadamente fuerte: las inferencias sobre las personas están protegidas incluso frente a un adversario que tiene pleno conocimiento del algoritmo DP, poder computacional ilimitado y *arbitrary side knowledge* sobre los datos de entrada. Además, DP satisface una serie de propiedades atractivas desde el punto de vista del diseño de algoritmos, incluida la conservación bajo procesamiento posterior y una degradación suave con múltiple accesos a los mismos datos. Estas propiedades se explotan en la construcción de algoritmos DP complejos basados en la combinación de pequeños bloques de construcción que inyectan ruido cuidadosamente calibrado en las operaciones que acceden a los datos. La magnitud del ruido requerido para satisfacer la garantía de privacidad aumenta con la fuerza de los parámetros de privacidad, lo que lleva a una compensación inevitable entre utilidad y privacidad, como lo ilustra la Ley Fundamental de Recuperación de Información [(Dwork and Roth, 2014)](https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf).

Juntos, la solidez de la garantía formal que brinda y la variedad de herramientas disponibles para la construcción de algoritmos de DP han llevado a la creciente adopción de DP como un estándar de oro para el aprendizaje automático que preserva la privacidad. Para problemas de *convex learning*, existe una variedad de métodos para obtener algoritmos diferencialmente privados, incluida la perturbación de salida [(Chaudhuri et al., 2011](https://www.jmlr.org/papers/volume12/chaudhuri11a/chaudhuri11a.pdf); [Wu et al., 2017](https://arxiv.org/pdf/1606.04722.pdf)), la perturbación objetiva ([Chaudhuri et al., 2011](https://www.jmlr.org/papers/volume12/chaudhuri11a/chaudhuri11a.pdf); [Kifer et al., 2012](http://proceedings.mlr.press/v23/kifer12/kifer12.pdf)) y la perturbación de gradiente ([Bassily et al., 2014](https://arxiv.org/pdf/1405.7085.pdf); [Song et al., 2013](https://cseweb.ucsd.edu/~kamalika/pubs/scs13.pdf)) .

La naturaleza de los problemas convexos permite el análisis formal de la utilidad de privacidad que ofrecen estos algoritmos, y en la actualidad existen grandes clases de problemas para los cuales se conocen algoritmos que logran (casi) equilibrios óptimos de utilidad de privacidad ([Asi et al., 2021](https://arxiv.org/pdf/2103.01516.pdf); [Bassily et al., 2014](https://arxiv.org/pdf/1405.7085.pdf); [Feldman et al., 2020](https://arxiv.org/pdf/2005.04763.pdf); [Song et al., 2021](https://proceedings.mlr.press/v130/song21a.html); [Talwar et al., 2015](https://papers.nips.cc/paper/2015/file/52d080a3e172c33fd6886a37e7288491-Paper.pdf)). Para los problemas de aprendizaje no convexos, la gama de algoritmos disponibles es más limitada y las ventajas y desventajas de la privacidad y la utilidad son más difíciles de analizar teóricamente. No obstante, para tales problemas existen dos familias de algoritmos que han demostrado lograr compensaciones razonables de privacidad-utilidad-cómputo en la práctica; perturbación de gradiente aplicada a optimizadores estándar como SGD ([Abadi et al., 2016](https://arxiv.org/pdf/1607.00133.pdf)),  y agregación privada de *teacher ensembles ([Papernot et al., 2018](https://arxiv.org/pdf/1802.08908.pdf)).* En este trabajo nos centramos en el primero, que es el más utilizado.

**2.2.** **Descenso de gradiente estocástico diferencialmente privado (DP-SGD)**

En este trabajo, suponemos que el algoritmo diferencialmente privado A, es un algoritmo de aprendizaje que mapea un conjunto de datos de entrenamiento: $D = \lbrace(x_i, y_i)\rbrace_{1\leq i\leq N}$ a un vector de parámetros de la red neuronal aprendida $w \in S = R^{p}$. Sea $\mathcal{L}(w,x,y)$ el objetivo de aprendizaje (por ejemplo, la pérdida de entropía cruzada), dados los parámetros del modelo $w$, la entrada ejemplo $x$ y la etiqueta $y$. Por comodidad, utilizamos la notación abreviada $l_i(w) = \mathcal{L}(w,x_i,y_i)$.

En la configuración non-private, una actualización de parámetros utilizando el Descenso de Gradiente Estocástico (SGD) en la iteración $t$ extrae $B$ ejemplos al azar del conjuntos de datos, y realiza una actualización de la forma:

$$
w^{(t+1)}= w^{(t)} - \eta_{t}\frac{1}{B} \sum_{i \in \beta_t}\nabla l_i(w^{(t)}),
$$

Donde $\eta_t$ es el step-size para una actualización $t^{th}$, $\nabla$ denota el operador de gradiente, y $B_t$ representa el conjunto de ejemplos muestreados en la iteración $t$ con $|B_t| = B$. Para que este algoritmo sea diferencialmente privado, aplicamos las siguientes modificaciones. En primer lugar, el gradiente de cada ejemplo del mini-batch es clipped a una norma máxima $C$, y en segundo lugar, se añade ruido gaussiano con desviación estándar proporcional a $C$ se añade a la media de los grandientes clipped. 

Sea 

$$
clip_x:v\in R^{p}\longmapsto min\lbrace{1,\frac{c}{||v||_2}\rbrace}.v\in R^{p} 
$$

denote la función de clipping que reescala su entrada para que la salida tenga una norma máxima $l_2$ de $C$. El nuevo step de actualización es:

$$
\begin{equation}  w^{(t+1)}= w^{(t)} -\eta_{t} \{\frac{1}{B}\sum_{i \in \beta_t}clip_c\left(\nabla l_i(w^{(t)})\right) +  \frac{\sigma C}{B}\xi\}, \end{equation}
$$

Donde $\xi \sim N(0,I_p)$ es una variable aleatoria gaussiana estándar de $p$ dimensiones y $\sigma$ especifica la desviación estándar del ruido añadido. El algoritmo resultante se llama ***Differentially Private-Stochastic Gradient Descent (DP-SGD)*** ([Abadi et al., 2016](https://arxiv.org/pdf/1607.00133.pdf)). Intuitivamente, realizar una actualización del modelo utilizando la ecuación proporciona privacidad diferencial porque la adición de ruido gaussiano con desviación estándar proporcional a $C$ es suficiente para enmascarar la contribución de cualquier ejemplo individual cuyo gradiente recortado tenga una norma menor o igual a $C$. Aunque utilizamos el SGD privatizado como nuestro optimizador a lo largo de este trabajo, también se puede utilizar un método de privatización similar en combinación con otros algoritmos de optimización de primer orden, como SGD con momentum o Adam ([McMahan et al., 2018a](https://arxiv.org/pdf/1812.06210.pdf)).

A lo largo de este trabajo utilizamos una versión modificada de DP-SGD en la que el gradiente privatizado está normalizado por $C$:

$$
\begin{equation} w^{(t+1)}= w^{(t)} -\eta_{t} \{\frac{1}{B}\sum_{i \in \beta_t}\frac{1}{C} clip_c\left(\nabla l_i(w^{(t)})\right) + \frac{\sigma}{B}\xi\}, \end{equation}
$$

Esta es una reparametrización de la ecuación (2) en la que la tasa de aprendizaje $\eta_t$ absorbe un factor de $C$. Esto no tiene ningún efecto sobre las garantías de privacidad, pero asegura que la norma de clipping no influya en la escala de la actualización, lo que simplifica el ajuste de los hiperparámetros. Tenga en cuenta que para preservar las garantías de DP, debemos dividir por $C$ después de la operación de clipping. El Apéndice A.1 proporciona más detalles sobre nuestra implementación de DP-SGD, incluida una descripción de nuestro enfoque de procesamiento por virtual batching para permitir el entrenamiento con grandes batch sizes.

**Privacy accounting.** La garantía de privacidad de DP-SGD está determinada por tres parámetros: la desviación estándar $\sigma$, el ratio de muestreo $q=B/N$ y el número de iteraciones de entrenamiento $T$. En la práctica, el presupuesto de privacidad $(\epsilon, \delta)$ suele ser fijo, y estos tres hiperparámetros se eligen para proporcionar el mejor rendimiento posible dentro de este presupuesto. También puede haber restricciones prácticas adicionales (por ejemplo, el presupuesto de cálculo máximo disponible). El proceso de calibración de la privacidad se realiza mediante un contador de privacidad: un algoritmo numérico que proporciona límites superiores ajustados para el presupuesto de privacidad en función de los hiperparámetros ([Abadi et al., 2016](https://arxiv.org/pdf/1607.00133.pdf)), que a su vez puede combinarse con rutinas de optimización numérica para optimizar un hipérparametro dado el presupuesto de privacidad y los otros dos hiperparámetros.

En este trabajo utilizamos el método de contabilidad para DP-SGD propuesto por M[ironov et al. (2019)](https://arxiv.org/pdf/1908.10530.pdf) e implementado en TensorFlow Privacy ([Google, 2018](https://github.com/tensorflow/privacy)). Esta privacidad contable se basa en un análisis de “composición” a través de iteraciones, que nos permite liberar no solo el modelo final, sino también cada modelo intermedio obtenido durante el entrenamiento (bajo el mismo presupuesto de privacidad)

**2.3. Retos de DP-SGD**

Como describimos anteriormente, existen tres diferencias clave entre DP-SGD y SGD no privado: (1) Los gradientes por ejemplo se recortan (clipped) a una norma máxima de $l_2$ antes de promediarlos, (2) se añade ruido gaussiano a la media de los gradientes clipped, y (3) el número máximo de actualizaciones permitidas dentro del presupuesto de privacidad está limitado y depende del batch size/ruido añadido. Estas diferencias plantean una serie de retos:

**Ajuste y regularización de hiperparámetros.** El ruido agregado a la estimación del gradiente en la actualización de DP-SGD (Ecuación (3)) es una barrera significativa para la optimización eficiente, y si reducimos la escala de este ruido, el número de iteraciones de entrenamiento permitidas dentro del presupuesto de privacidad disminuye. Esta restricción altera los valores óptimos de hiperparámetros clave como el batch size/learning rate, y los valores por defecto del entrenamiento no privado pueden ser muy subóptimos ([Papernot et al., 2021](https://arxiv.org/pdf/2007.14191.pdf)). En consecuencia, DP-SGD requiere un ajuste cuidadoso de los hiperparámetros.

En nuestros experimentos también descubrimos que, al entrenar con DP-SGD, las mejoras en el training accuracy suelen traducirse directamente en una mejora de la generalización, sin necesidad de una fuerte regularización. Inspirados en esta observación, nuestra filosofía es que los métodos que reducen el número de iteraciones de entrenamiento necesarias para alcanzar un high training accuracy en el entrenamiento no privado probablemente mejoren el test accuracy alcanzado en el entrenamiento privado. De acuerdo con este enfoque, suele ser beneficioso eliminar los métodos de regularización explícitos.

**Bias y Varianza de la actualización DP-SGD.**  El estimador de gradiente utilizado por DP-SGD está biased debido al uso de gradiente clipping por ejemplo y, en general, no corresponde al gradiente de ninguna función diferenciable ([Song et al., 2021](https://proceedings.mlr.press/v130/song21a.html)). Y lo que es más importante, la norma de clipping $C$ introduce una compensación entre sesgo y varianza ([Chen et al., 2020](https://arxiv.org/pdf/2006.15429.pdf); [Thakkar et al., 2021](https://arxiv.org/pdf/1905.03871.pdf)).  Esto puede verse en la actualización DP-SGD que se muestra en la ecuación (3). Cuando $C$ es muy grande, $clip_c$ es la función identidad, por lo que el gradiente privatizado es un estimador unbiased del verdadero gradiente, pero el gradiente clipped $\frac {1}{c} clip_c(\nabla l_i(w^{(t)}))$ es muy pequeño en comparación con el ruido (que es independiente de $C$) - en general, la estimación del gradiente privatizado tiene un bias bajo y una varianza alta. Por el contrario, si $C$ es pequeño, la operación de clipping introduce un bias, pero el gradiente clipped $\frac {1}{c} clip_c(\nabla l_i(w^{(t)}))$ es mayor, y por lo tanto no es necesariamente pequeño en comparacion con el ruido- en general, la estimación del gradiente privatizado tiene un bias alto y una varianza baja. Tenga en cuenta que cuando $C$ es muy pequeño (más pequeño que la norma de gradiente más pequeña por ejemplo), reducir aún más $C$ no cambia $\frac {1}{c} clip_c(\nabla l_i(w^{(t)}))$, lo que indica que el bias y la varianza en la actualización se aproximan a una constante a medida que $C \longmapsto 0$.  Curiosamente, trabajos anteriores han observado que amplios rangos de la norma de clipping $C$ pueden proporcionar un rendimiento casi óptimo siempre que (i) la norma de clipping sea lo suficientemente pequeña, y (ii) el learning-rate se reescalen en consecuecia [(Kurakin et al., 2022](https://arxiv.org/pdf/2201.12328.pdf); [Li et al., 2021](https://arxiv.org/pdf/2110.05679.pdf)). Esto sugiere que reducir la varianza introducida por el ruido puede ser más importante que reducir el bias introducido por el clipping.

**Hacer que los modelos estándar funcionen.** El entrenamiento diferencialmente privado ha obtenido recientemente resultados prometedores con arquitecturas estándar en NLP, tanto al entrenar un modelo BERT ([Devlin et al., 2018](https://arxiv.org/pdf/1810.04805.pdf)) a partir de una inicialización aleatoria ([Anil et al., 2018](https://arxiv.org/pdf/2108.01624.pdf)) como al afinar un gran modelo de lenguaje Transformer ([Vaswani et al., 2017](https://arxiv.org/pdf/1706.03762.pdf)) a partir de un conjunto de parámetros preentrenados ([Li et al., 2021](https://arxiv.org/pdf/2110.05679.pdf); [Yu et al., 2021a](https://arxiv.org/pdf/2110.06500.pdf)). Sin embargo, no se han obtenido resultados similares en visión por ordenador, y la bibliografía no ofrece recomendaciones claras sobre qué arquitecturas de modelos funcionan bien. Por ejemplo, analizando la investigación reciente sobre el entrenamiento privado para CIFAR-10, [Kurakin et al. (2022)](https://arxiv.org/pdf/2201.12328.pdf), [Papernot et al. (2021)](https://arxiv.org/pdf/2007.14191.pdf) y [Dörmann et al. (2021)](https://arxiv.org/pdf/2110.06255.pdf) utilizan variantes de modelos VGG poco profundos ([Simonyan y Zisserman, 2015](https://arxiv.org/pdf/1409.1556.pdf)), mientras que [Tramèr y Boneh (2021)](https://arxiv.org/pdf/2011.11660.pdf) utilizan ScatterNets ([Oyallon y Mallat, 2015](https://arxiv.org/pdf/1412.8659.pdf)) para entrenar modelos lineales en características artesanales, logrando una impresionante precisión de prueba del 69,3 % con un presupuesto de privacidad ajustado de $(3, 10^{-5})-DP$. Por último, [Klause et al. (2022)](https://arxiv.org/pdf/2203.00324.pdf) logran la precisión de prueba SOTA para 𝜀 ≤ 8 sin datos adicionales del 71,7% al entrenar una red residual superficial de 9 capas [(He et al., 2016](https://arxiv.org/pdf/1512.03385.pdf)) bajo $(7,5, 10^{-5})-DP$.

La norma $l_2$ del ruido añadido en la actualización DP-SGD escala proporcionalmente a la dimensión del gradiente (el número de parámetros). Esta observación ha llevado a muchos investigadores a creer que los modelos estándar sobreparametrizados funcionarán mal con DP-SGD, y en su lugar se centran en reducir la dimensión explícita o implícita de la actualización, ya sea mediante el uso de modelos pequeños/hand-crafted features ([Tramèr y Boneh, 2021](https://arxiv.org/pdf/2011.11660.pdf)) o mediante técnicas de reducción de la dimensionalidad ([Yu et al., 2021b,c](https://arxiv.org/pdf/2102.12677.pdf)). Otro obstáculo clave para el uso de modelos estándar para el entrenamiento privado en visión por computador ha sido que, con el fin de proporcionar garantías ajustadas de DP, DP-SGD requiere que los gradientes evaluados en diferentes ejemplos de entrenamiento sean independientes. Esto excluye el uso de cualquier método que permita la comunicación entre ejemplos de entrenamiento, como batch normalization ([Ioffe y Szegedy, 2015](https://arxiv.org/pdf/1502.03167.pdf)), que hasta hace poco ha sido casi omnipresente en las arquitecturas de visión estándar ([Brock et al., 2021b](https://arxiv.org/pdf/2102.06171.pdf); [Dosovitskiy et al., 2020](https://arxiv.org/pdf/2010.11929.pdf); [He et al., 2016](https://arxiv.org/pdf/1512.03385.pdf); [Tan y Le, 2019](https://arxiv.org/pdf/1905.11946.pdf); [Zagoruyko y Komodakis, 2016](https://arxiv.org/pdf/1605.07146.pdf)).